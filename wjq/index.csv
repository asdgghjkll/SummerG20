ä¿¡æ¯ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢2020çº§è€ƒç ”è€ƒå…¬å­¦ç”Ÿæš‘æœŸç•™å®¿ä½è¯ææ–™.docx,D:/tool/Pycharm/summerProject/examples\ä¿¡æ¯ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢2020çº§è€ƒç ”è€ƒå…¬å­¦ç”Ÿæš‘æœŸç•™å®¿ä½è¯ææ–™.docx,"æ¯ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢2020çº§è€ƒç ”ï¼ˆè€ƒå…¬ï¼‰å­¦ç”Ÿæš‘æœŸç•™å®¿ä½è¯ææ–™

"
åŒ—èˆªå­¦æŠ¥-åŸºäºGANçš„é«˜åŠ¨æ€èŒƒå›´å›¾åƒç”Ÿæˆæ–¹æ³•.pdf,D:/tool/Pycharm/summerProject/examples\åŒ—èˆªå­¦æŠ¥-åŸºäºGANçš„é«˜åŠ¨æ€èŒƒå›´å›¾åƒç”Ÿæˆæ–¹æ³•.pdf,"åŒ— äº¬ èˆª ç©º èˆª å¤© å¤§ å­¦ å­¦ æŠ¥ 
Journal of Beijing University of Aeronautics and Astronautics  
 
æ”¶ç¨¿æ—¥æœŸ : 2020-09-14ï¼› å½•ç”¨æ—¥æœŸ : 2021-04-23; ç½‘ç»œå‡ºç‰ˆæ—¶é—´ : 2021 -05-17 17:00  
ç½‘ç»œå‡ºç‰ˆåœ°å€ : kns.cnki.net/kcms/detail/11.2625. V.20210517.1523.003.html  
åŸºé‡‘é¡¹ç›®: æµ™æ±Ÿçœè‡ªç„¶ ç§‘å­¦åŸºé‡‘ï¼ˆ LY20F010013ï¼‰  
*é€šè®¯ä½œè€… : E-mailï¼š DandanDing@hznu.edu.c n   
å¼•ç”¨æ ¼å¼ ï¼š 
 
 
  http://bhxb.buaa.edu.cn  jbuaa@buaa.edu.cn  
 DOIï¼š10.13700/j.bh.1001 -5965. 2020.0518  
åŸºäºæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ HDRå›¾åƒç”Ÿæˆæ–¹æ³•  
è´æ‚¦1ï¼Œç‹ç¦1ï¼Œç¨‹å¿—é¹1ï¼Œæ½˜å…´æµ©1ï¼Œæ¨é»˜æ¶µ2ï¼Œä¸ä¸¹ä¸¹2,*  
ï¼ˆ1ï¼å’ªå’•è§†è®¯ç§‘æŠ€æœ‰é™å…¬å¸ ,  ä¸Šæµ·   201201ï¼›2ï¼åŒ—äº¬è§‚æ­¢åˆ›æƒ³ç§‘æŠ€æœ‰é™å…¬å¸ ,  åŒ—äº¬   100036)  
æ‘˜      è¦ï¼šé«˜åŠ¨æ€èŒƒå›´ (HDR)å›¾åƒç›¸æ¯”ä½åŠ¨æ€èŒƒå›´ (LDR)å›¾åƒæœ‰æ›´å®½çš„è‰²åŸŸå’Œæ›´é«˜çš„äº®åº¦èŒƒå›´ï¼Œæ›´
ç¬¦åˆäººçœ¼è§†è§‰æ•ˆæœ ï¼Œ ä½†ç”±äºç›®å‰çš„å›¾åƒé‡‡é›†è®¾å¤‡å¤§éƒ½æ˜¯ LDRè®¾å¤‡ï¼Œå¯¼è‡´ HDRå›¾åƒèµ„æºåŒ®ä¹ï¼Œè§£å†³è¯¥
é—®é¢˜çš„ä¸€ç§æœ‰æ•ˆé€”å¾„æ˜¯é€šè¿‡é€†è‰²è°ƒæ˜ å°„å°† LDRå›¾åƒæ˜ å°„ä¸º HDRå›¾åƒã€‚æå‡º äº†ä¸€ç§åŸºäºæ¡ä»¶ ç”Ÿæˆå¯¹æŠ—ç½‘
ç»œ(CGAN)çš„é€†è‰²è°ƒæ˜ å°„ç®—æ³•ï¼Œä»¥é‡å»º HDRå›¾åƒã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†åŸºäºå¤šåˆ†æ”¯çš„ç”Ÿæˆ å¯¹æŠ—ç½‘ç»œä¸åŸºäºé‰´åˆ«
å—çš„é‰´åˆ«ç½‘ç»œï¼Œå¹¶åˆ©ç”¨ CGANçš„æ•°æ®ç”Ÿæˆèƒ½åŠ›å’Œç‰¹å¾æå–èƒ½åŠ›ï¼Œå°†å•å¼  LDRå›¾åƒä» BT.709è‰²åŸŸæ˜ å°„åˆ°
å¯¹åº”çš„ BT.2020è‰²åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„ç½‘ç»œèƒ½å¤Ÿè·å¾—æ›´é«˜çš„å®¢è§‚ä¸ä¸»è§‚è´¨é‡ ï¼Œ
ç‰¹åˆ«æ˜¯é’ˆå¯¹ä½è‰²åŸŸä¸­çš„æ¨¡ç³ŠåŒºåŸŸï¼Œæ‰€ææ–¹æ³•èƒ½å¤Ÿé‡å»ºå‡ºæ›´æ¸…æ™°çš„çº¹ç†ä¸ç»†èŠ‚ã€‚  
å…³  é”®  è¯ï¼šæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ ; å·ç§¯ç¥ç»ç½‘ç»œ ; é€†è‰²è°ƒæ˜ å°„ ; è‰²åŸŸè½¬æ¢ ; ç‰¹å¾æå–  
ä¸­å›¾åˆ†ç±»å·ï¼š TP391                       
æ–‡çŒ®æ ‡è¯†ç ï¼š A                           
 
 
 
éšç€æ˜¾ç¤ºè®¾å¤‡æŠ€æœ¯äº§ä¸šçš„å‘å±•ï¼Œæ™®é€š ä½åŠ¨æ€
èŒƒå›´ (Low  Dynamic Range ï¼ŒLDR)çš„æ˜¾ç¤ºå™¨é€æ¸ä¸
èƒ½æ»¡è¶³å¤§ä¼—å½±éŸ³éœ€æ±‚ï¼Œå…·æœ‰æ›´é«˜äº®åº¦ã€æ›´å¹¿è‰²åŸŸ
çš„é«˜åŠ¨æ€èŒƒå›´ (High  Dynamic Range ï¼ŒHDR)æ˜¾ç¤ºè®¾
å¤‡é€æ¸å‡ºç°ã€‚ä¸æ™®é€šçš„ LDRå›¾åƒç›¸æ¯”ï¼Œ HDRå›¾
åƒçš„ç¼–ç ä½æ•°ç”± 8 bitæå‡è‡³ 10 bitæˆ–ä»¥ä¸Šï¼Œå›¾åƒ
è‰²åŸŸèŒƒå›´ç”± BT.709æ ‡å‡†æå‡è‡³ BT.2020æ ‡å‡†ï¼Œ èƒ½
æä¾›æ›´å¤šçš„åŠ¨æ€èŒƒå›´å’Œå›¾åƒç»†èŠ‚ ã€‚ç„¶è€Œï¼Œç›®å‰ä¸
HDRæ˜¾ç¤ºè®¾å¤‡ç›¸é€‚é…çš„è§†é¢‘å›¾åƒèµ„æºååˆ†åŒ®ä¹ã€‚
å› æ­¤ï¼Œé€šè¿‡é€†è‰²è°ƒæ˜ å°„ï¼Œå°†åŸæœ¬çš„ LDRå›¾åƒç”Ÿæˆ
å¯¹åº”çš„é«˜è´¨é‡ HDRå›¾åƒï¼Œä¹Ÿå°±æ˜¯é€†è‰²è°ƒæ˜ å°„ï¼Œ
æ˜¯æœ‰æ•ˆæå‡åŸæœ‰å›¾åƒè§†é¢‘è´¨é‡çš„æœ‰æ•ˆé€”å¾„ä¹‹ä¸€
[1]ã€‚ 
ä»LDRå›¾åƒåˆ° HDRå›¾åƒçš„æ˜ å°„æ˜¯ä¸€ä¸ªéçº¿
æ€§çš„ã€ä¸é€‚å®šé—®é¢˜ï¼Œå­˜åœ¨å¤šç§è§£ã€‚ä¸ LDRå›¾åƒç›¸
æ¯”ï¼Œ HDRå›¾åƒè‰²åŸŸæ›´å¹¿ï¼Œå¹¿ä¹‰ä¸Šçš„ HDRå›¾åƒè‰²
åŸŸåº”èƒ½è¦†ç›–åˆ° 90%ä»¥ä¸Šçš„ DCI-P3è‰²åŸŸèŒƒå›´ ; ä½
æ·±ç”± LDRå›¾åƒçš„ 8 bitå¢åŠ åˆ° 10 bitã€‚å› æ­¤ï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹ï¼Œç”± LDRå›¾åƒç”Ÿæˆå¯¹åº”çš„ HDRå›¾åƒéœ€
è¦è‰¯å¥½çš„å¯¹åº”ç‚¹åƒç´ ç”Ÿæˆèƒ½åŠ›ã€‚ç”±æ­¤å¯å‘å¯ä»¥ä½¿
ç”¨ç¥ç»ç½‘ç»œæ¥å®Œæˆä¸Šè¿°é€†æ˜ å°„ã€‚  
å· ç§¯ ç¥ ç» ç½‘ ç»œ ï¼ˆ Convolutional Neural 
Networks ï¼ŒCNNï¼‰é¦–å…ˆè¢«ç”¨äºä¸Šè¿°é€†è‰²è°ƒæ˜ å°„ï¼Œ
ä»¥å®Œæˆ HDRé‡å»ºã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œ ä¸€äº›åŸºäº CNN
çš„æ–¹æ³•èƒ½å¤Ÿ è·å¾—æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜çš„æ€§èƒ½[]-[7]ã€‚ä½†
æ˜¯ï¼Œè¿™äº›åŸºäº CNNçš„æ–¹æ³•é€šå¸¸æ¶‰åŠå¤§é‡å‚æ•°ï¼Œ
å› è€Œåœ¨è®­ç»ƒä¸­ä¼šèŠ±è´¹å¾ˆé•¿æ—¶é—´ã€‚æ­¤å¤–ï¼Œåœ¨ç½‘ç»œè®­
ç»ƒä¸­å¾€å¾€ä½¿ç”¨ L2æŸå¤±å‡½æ•°ï¼Œè™½ç„¶è·å¾—äº†è¾ƒé«˜çš„
å³°å€¼ä¿¡å™ªæ¯” (Peak Signal -to-Noise Ratio ï¼ŒPSNR)ï¼Œ
ä½†å›¾åƒä¸»è§‚è´¨é‡åäºå¹³æ»‘ï¼Œé™ä½äº† HDRçš„è§‚çœ‹
ä½“éªŒã€‚è¿‘ å¹´æ¥ï¼Œä¸ºå…‹æœä¸Šè¿°é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡º
ä½¿ç”¨åŸºäºå¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆ Generative Adversarial 
Networkï¼ŒGANï¼‰çš„ HDRé‡å»ºæ–¹æ³•ã€‚ ä»¥å¾€å·¥ä½œ[]-[11]
ä¸€èˆ¬éƒ½ä½¿ç”¨ U-netåšä¸º GANçš„ç”Ÿæˆå™¨ï¼Œ ä½¿ç”¨å·ç§¯
é…åˆé™é‡‡æ ·çš„ç»“æ„ä½œä¸ºé‰´åˆ«å™¨ï¼Œ æé«˜äº†é‡å»º HDR
çš„ä¸»è§‚æ€§èƒ½ã€‚è¿™äº›ç ”ç©¶è¡¨æ˜äº† GANç½‘ç»œåœ¨ HDR2                                                      åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦å­¦æŠ¥   
[é”®å…¥æ–‡å­— ] 
 é‡å»ºæ–¹é¢å…·æœ‰å·¨å¤§ çš„æ½œåŠ›ã€‚ GANç½‘ç»œæ— éœ€è¦æ±‚ä¸€
ä¸ªå‡è®¾çš„æ•°æ®åˆ†å¸ƒï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç§åˆ†å¸ƒç›´æ¥è¿›è¡Œ
é‡‡æ ·ï¼Œä»è€Œè¾¾åˆ°ç†è®ºä¸Šå¯ä»¥å®Œå…¨é€¼è¿‘çœŸå®æ•°æ®ï¼Œ
è¿™ä¹Ÿæ˜¯ GANç½‘ç»œæœ€å¤§çš„ä¼˜åŠ¿ã€‚ä½†æ˜¯ï¼Œ GANç½‘ç»œ
æ— éœ€é¢„å…ˆå»ºæ¨¡çš„æ–¹æ³•å¯¹äºè¾ƒå¤§å›¾åƒè€Œè¨€ï¼Œç”Ÿæˆç›®
æ ‡çš„å¯æ§æ€§ä¸å¼ºï¼Œ æ— æ³•ç”Ÿæˆå…·æœ‰ç‰¹å®šæ€§è´¨çš„å›¾åƒã€‚  
å—ä¸Šè¿°å·¥ä½œçš„å¯å‘ï¼Œ æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¡ä»¶
ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Conditional Generative Adversarial 
Networkï¼ŒCGAN)çš„ä»å•å¼  LDRå›¾åƒç”Ÿæˆå¯¹åº”
HDRå›¾åƒçš„æ–¹æ³•ã€‚åˆ©ç”¨ CGANç½‘ç»œçš„æ€æƒ³ï¼Œ é€šè¿‡
åœ¨GANç½‘ç»œçš„ç”Ÿæˆç½‘ç»œå’Œé‰´åˆ«ç½‘ç»œä¸­æ·»åŠ æ¡ä»¶
æ€§çš„çº¦æŸï¼Œæ¥è§£å†³è®­ç»ƒè¿‡äºè‡ªç”±çš„é—®é¢˜ï¼Œå® ç°äº†
ç”±å•å¼  LDRå›¾åƒåˆ°å¯¹åº” HDRå›¾åƒçš„ç«¯åˆ°ç«¯çš„é
çº¿æ€§æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ æœ¬æ–‡æ–¹æ³•ä¸ä»…èƒ½å¤Ÿå°† 8 
bitçš„LDRå›¾åƒè‡ªç„¶åœ°æ˜ å°„åˆ° 10 bitçš„HDRå›¾åƒï¼Œ è¿˜
èƒ½æŒ–æ˜ LDRå›¾åƒä¸­çš„ä¸€äº›éšè—ç‰¹å¾ï¼Œä¸ä»¥å¾€æ–¹æ³•
ç›¸æ¯”ï¼Œæ‰€ææ–¹æ³•è·å¾—äº†è¾ƒé«˜çš„ä¸»è§‚ä¸å®¢è§‚è´¨é‡ã€‚  
1  ç›¸å…³å·¥ä½œ  
åŸºäºç¥ç»ç½‘ç»œæ–¹æ³•ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è§£å†³é€†è‰²
è°ƒæ˜ å°„é—®é¢˜å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚  
1.1 å¤šæ›å…‰çš„æ–¹æ³•  
å¤šæ›å…‰çš„æ–¹æ³•æ˜¯å¤„ç†é€†è‰²è°ƒæ˜ å°„çš„æœ€ç›´æ¥æ–¹
æ³•ï¼Œå…¶åˆå¹¶ä¸åŒæ›å…‰æ‹æ‘„çš„å¤šä¸ª LDRå›¾åƒï¼Œæœ€ç»ˆ
ç”Ÿæˆå•ä¸ª HDRå›¾åƒã€‚ä¸è¿‡ï¼Œä¸€èˆ¬å¤šæ›å…‰çš„å›¾åƒ
è·å–éœ€è¦ä½¿ç”¨ä¸“ä¸šç›¸æœºï¼Œè¿™å¯¹äºå®é™…åº”ç”¨å¹¶ ä¸å‹
å¥½ï¼Œå› ä¸ºå®é™…åº”ç”¨ä¸­æ‰€æ¶‰åŠçš„å›¾åƒå‡ ä¹éƒ½æ˜¯éä¸“
ä¸šç”¨æˆ·æ‹æ‘„çš„å•æ›å…‰å›¾åƒã€‚ä¸ºæ­¤ï¼Œæ™®éé‡‡ç”¨çš„æ–¹
å¼æ˜¯ï¼šé¦–å…ˆï¼Œä½¿ç”¨è½¯ä»¶ç®—æ³•ä»å•ä¸ªè¾“å…¥ LDRå›¾åƒ
ç”Ÿæˆè‹¥å¹²ä¸ªä¸åŒæ›å…‰çš„ LDRå›¾åƒï¼›ç„¶åï¼Œä½¿ç”¨è¿™
äº›ç”Ÿæˆçš„ LDRå›¾åƒé‡å»ºå‡ºå¯¹åº”çš„ HDRå›¾åƒã€‚å¦‚
æ­¤ï¼Œå¤šæ›å…‰æ–¹æ³•ä¸€èˆ¬åŒ…æ‹¬ 2ä¸ªæ­¥éª¤ï¼Œå³ç”Ÿæˆå’Œé‡
å»ºï¼Œä¸”æ¯ä¸ªæ­¥éª¤éƒ½å¯ä»¥ç”±ç¥ç»ç½‘ç»œæ¥å®ç°ã€‚  
åŸºäºä¸Šè¿°å¤šæ›å…‰çš„æ€æƒ³ï¼Œ Endoç­‰[2]å¼€å‘äº†ä¸€
å¯¹åŸºäºâ€œç¼–ç å™¨ -è§£ç å™¨â€ç»“æ„çš„ CNNç½‘ç»œ
DrTMOï¼ŒåŒ…æ‹¬å‘ä¸Šæ›å…‰æ¨¡å‹å’Œå‘ä¸‹æ›å…‰æ¨¡å‹ï¼Œåˆ†
åˆ«æ¨æ–­ 2ç»„æ›å…‰çš„ LDRå›¾åƒï¼Œ ä½¿ç”¨å·²æœ‰çš„åˆå¹¶æ–¹
æ³•å°†è¿™äº› LDRå›¾åƒåˆå¹¶ä¸ºä¸€ä¸ª HDRå›¾åƒã€‚ Lee
ç­‰[7]æå‡ºäº†ä¸€ä¸ª CNNé“¾å¼ç»“æ„ï¼Œ é‡‡ç”¨ 2ä¸ªå¹¶è¡Œçš„
CNNï¼Œæ¯ä¸ª CNNæ¨æ–­ 3ä¸ªLDRå›¾åƒã€‚åæ¥ ï¼ŒLee
ç­‰[10]åŸºäº GANçš„ç»“æ„è¿›ä¸€æ­¥æ”¹è¿›äº†è®¾è®¡ï¼Œä½¿ç”¨
2ä¸ªç¥ç»ç½‘ç»œ G_pluså’ŒG_minus æ¥ç”Ÿæˆæ›å…‰å›¾
åƒã€‚Xuç­‰[8]å°†GANç”¨äº LDRå›¾åƒçš„åˆå¹¶ï¼Œå…ˆåˆ©
ç”¨ç›¸æœºå“åº”æ›²çº¿æ¥æ›´æ”¹è¾“å…¥ LDRå›¾åƒçš„æ›å…‰æ—¶é—´ï¼Œ å¹¶è·å¾— 2ä¸ªæ›å…‰è¿‡åº¦å’Œ 2ä¸ªæ›å…‰ä¸è¶³çš„å›¾åƒï¼Œ
è®­ç»ƒäº†ä¸€ä¸ª GANç½‘ç»œæ¥åˆå¹¶è¿™äº› LDRå›¾åƒä»¥è¿›
è¡ŒHDRå›¾åƒé‡å»ºã€‚  
1.2 å•æ›å…‰çš„æ–¹æ³•  
ç›¸å¯¹äºå¤šæ›å…‰ çš„æ–¹æ³•ï¼Œå•æ›å…‰ çš„æ–¹æ³•å°†è¾“å…¥
çš„LDRå›¾åƒç›´æ¥è½¬æ¢ä¸ºè¾“å‡ºçš„ HDRå›¾åƒï¼Œçµæ´»æ€§
æ›´é«˜ã€‚  
1.2.1 åŸºäº CNNçš„å•æ›å…‰  
Takeuchiç­‰[12]æå‡ºäº†ä¸€ç§åŸºäº CNNçš„ã€ä»
BT.709åˆ°BT.2020çš„è‰²å½©ç©ºé—´è½¬æ¢æ–¹æ³•ã€‚ Eilertsen
ç­‰[3]åŸºäºè‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œé‡æ„è¾“å…¥ LDRå›¾åƒä¸­çš„
è¿‡é¥±å’ŒåŒºåŸŸï¼Œæ‰€æå‡ºçš„ HDRCNN æ–¹æ³•åœ¨å›¾åƒä¸­çš„
è¿‡é¥±å’Œåƒç´ æ¯”ä¾‹ä½äº 5ï¼…æ—¶ï¼Œå¯ä»¥è¾¾åˆ°è¾ƒå¥½ çš„æ•ˆ
æœã€‚ Marnerides ç­‰[5]è®¾è®¡äº†ä¸€ä¸ª ExpandNetï¼ŒLDR
å›¾åƒé€šè¿‡å±€éƒ¨ ã€ä¸­ç­‰å’Œå…¨å±€ 3ç§CNNåˆ†æ”¯åè¿›è¡Œ
èåˆï¼Œå¾—åˆ°å¯¹åº”çš„ HDRå›¾åƒã€‚ ExpandNet èƒ½å¤Ÿæ
å‡ä¸€äº›å›¾ç‰‡çš„ä¸»è§‚è´¨é‡ï¼Œä½†åœ¨æŸäº›å›¾åƒä¸Šï¼Œä¼šå‡º
ç°é¢œè‰²åç§»æˆ–è€…ä¸»è§‚è´¨é‡ä¸é«˜ç­‰é—®é¢˜ã€‚ Kinoshita
å’ŒKiya[6]åœ¨ç°æœ‰ U-netçš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº†ä¸€ä¸ªå…¨å±€
ç¼–ç å™¨ï¼Œä»¥å’Œç°æœ‰çš„ U-netç¼–ç å™¨å¹¶è¡Œå·¥ä½œï¼Œæ¥è‡ª
2ä¸ªç¼–ç å™¨çš„ç»“æœè¢«ä¸€èµ·å¹¶é€å…¥ U-netè§£ç å™¨è¿›è¡Œ
HDRé‡å»ºã€‚  
1.2.2 åŸºäº GANçš„å•æ›å…‰  
GANç½‘ç»œå·²ç»åœ¨å¾ˆå¤šæ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›
å±•ï¼Œå¦‚å›¾åƒè¶…åˆ†è¾¨ç‡[]-[14]ã€å›¾åƒé£æ ¼è¿ç§»[]-[16]ã€
äººä½“å§¿æ€ä¼°è®¡[17]ã€äººä½“è¿åŠ¨ä¼ é€’[18]ç­‰ã€‚åœ¨é€†è‰²è°ƒ
æ˜ å°„æ–¹é¢ï¼Œ GANç½‘ç»œåˆšåˆšèµ·æ­¥ã€‚ Ningç­‰[9]åŸºäº
U-Netè®¾è®¡äº†ç”Ÿæˆå™¨ï¼Œå¹¶é‡‡ç”¨äº†åŸºäº CNNçš„é‰´åˆ«
å™¨ã€‚ Leeç­‰[10]åŸºäº GANè¿›è¡Œè¶…åˆ†è¾¨ç‡ä¸ HDRçš„è”
åˆé‡å»ºã€‚ ä»¥ä¸Šæ–¹æ³•éƒ½æ˜¯åŸºäºç°æˆçš„ç½‘ç»œå±•å¼€ï¼Œé’ˆ
å¯¹é€†è‰²è°ƒæ˜ å°„ï¼Œæœ‰å¿…è¦è®¾è®¡æ–°çš„ç½‘ç»œç»“æ„é…åˆæ–°
çš„æŸå¤±å‡½æ•°ï¼Œä»¥è·å¾—æ›´å¥½çš„é‡å»ºæ•ˆæœã€‚  
1.3 GANç½‘ç»œå’Œ CGANç½‘ç»œ  
GANç½‘ç»œç”± Goodfellow ç­‰[19]æå‡ºï¼Œæ˜¯ä¸€ç§åŸº
äºå¯¹æŠ—ç”Ÿæˆçš„åšå¼ˆæ€æƒ³åˆ›é€ å‡ºçš„æ¨¡å‹ï¼Œ å…¶ç»„æˆæœ‰ 2
ä¸ªç½‘ç»œï¼Œåˆ†åˆ«ä¸ºç”Ÿæˆç½‘ç»œ Gå’Œé‰´åˆ«ç½‘ç»œ Dã€‚ç”Ÿæˆç½‘
ç»œå’Œé‰´åˆ«ç½‘ç»œç›¸äº’å¯¹æŠ—ï¼Œæ‰€é‡‡ç”¨çš„ æ€æƒ³æ˜¯é›¶å’Œåš
å¼ˆæ€æƒ³ï¼Œå³åŒæ–¹åœ¨å¹³ç­‰çš„æƒ…å†µä¸‹å¼€å±€ï¼Œå„è‡ªä½¿ç”¨
å„è‡ªçš„ç­–ç•¥å»ä¸å¯¹æ–¹å¯¹æŠ—ï¼Œåœ¨å¯¹æŠ—ä¸­åŒæ–¹å†åˆ©ç”¨
å¯¹æ–¹çš„ç­–ç•¥åŠæ—¶å¯¹è‡ªå·±çš„ç­–ç•¥è¿›è¡Œè°ƒæ•´ã€‚åŒæ–¹å……
åˆ†éµå¾ªâ€œæŸäººåˆ©å·±â€å»å®ç°è·å¾—èƒœåˆ©çš„ç›®çš„ã€‚è¿™ä¸ª
æ€æƒ³æ‹“å±•åˆ°æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œå³ç”Ÿæˆç½‘ç»œå’Œé‰´åˆ«ç½‘
ç»œä¸ºåšå¼ˆçš„åŒæ–¹ï¼Œç”Ÿæˆç½‘ç»œä¸æ–­å‘æŒ¥è‡ªå·±æ‹Ÿåˆæ•°                                             è´æ‚¦ç­‰ï¼š åŸºäºæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ HDRå›¾åƒç”Ÿæˆæ–¹æ³•                                 
3 
 
 æ®çš„èƒ½åŠ›å»æ¬ºéª—é‰´åˆ«ç½‘ç»œï¼Œè€Œé‰´åˆ«ç½‘ç»œåˆ™å°½åŠ›å»
è¾¨åˆ«ç”Ÿæˆç½‘ç»œä¼  é€æ¥çš„æ‹Ÿåˆæ•°æ®ï¼Œä¸¤è€…çš„æœ€ç»ˆç›®
æ ‡æ˜¯è¾¾åˆ°çº³ä»€å¹³è¡¡[20]ï¼Œæœ€ç»ˆè®©ç”Ÿæˆç½‘ç»œå­¦ä¹ åˆ°çœŸ
å®çš„æ•°æ®åˆ†å¸ƒã€‚  
å¦‚å›¾ 1æ‰€ç¤ºï¼Œç”Ÿæˆç½‘ç»œ Gæ ¹æ®ç»™å‡ºçš„æ ‡ç­¾æ ·æœ¬
çš„æ•°æ®åˆ†å¸ƒï¼Œå°†æœä»éšæœºåˆ†å¸ƒçš„å™ªå£°
z åŒ…è£…æˆçœŸ
å®çš„æ ·æœ¬æ•°æ®ï¼Œ è¯¥è¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸ºç”Ÿæˆæ ·æœ¬æ•°æ®çš„
è¿‡ç¨‹ã€‚ç”Ÿæˆç½‘ç»œç”¨ è¯¥æ ·æœ¬æ•°æ®å»æ¬ºéª—é‰´åˆ«ç½‘ç»œã€‚
é‰´åˆ«ç½‘ç»œ Dåˆ™ç”¨æ¥é‰´åˆ«ç”Ÿæˆç½‘ç»œè¾“å‡ºçš„æ ·æœ¬æ•°æ®
çš„çœŸå‡æ€§ã€‚  
 
å›¾1  GANç½‘ç»œåŸºæœ¬ç»“æ„  
Fig.1  Basic structure of GAN  network   
ç”Ÿæˆå™¨ä¸ºäº†èƒ½å¤Ÿå­¦ä¹ çœŸå®çš„æ ·æœ¬åˆ†å¸ƒ
data()P x
ï¼Œå…ˆé€šè¿‡éšæœºåˆ†å¸ƒçš„å™ªå£°
z ï¼ˆä¸€èˆ¬è®¾ç½®ä¸º
é«˜æ–¯åˆ†å¸ƒï¼‰åˆ›å»ºæ˜ å°„
( ; )g Gï±z ï¼Œå†é€šè¿‡é‰´åˆ«å™¨çš„æ˜ 
å°„
( ; )d Dï±x åˆ¤æ–­æ‰€ç”Ÿæˆçš„æ•°æ®æ˜¯å¦ç¬¦åˆçœŸå®æ ·æœ¬
çš„åˆ†å¸ƒï¼Œå…¶ä¸­ Gã€Dæ˜¯ç”±å‚æ•°ä¸º Î¸gã€Î¸dçš„ç¥ç»ç½‘
ç»œè¡¨ç¤ºçš„å¾®åˆ†å‡½æ•° ã€‚åœ¨ GANç½‘ç»œä¸­ï¼Œé€šå¸¸é‡‡ç”¨
çš„ä»£ä»·å‡½æ•°ä¸º  
data ) ()D
(min max ( , )
([ (log  log 1 [ )] ( ( ( )))])PPGV D G
E D E D Gï¾ï¾ +âˆ’=
z x x z z xz
ï¼ˆ1ï¼‰ 
å¼ä¸­ï¼š
xä¸ºçœŸå®æ•°æ®åˆ†å¸ƒ ï¼›æœ€å¤§åŒ–
()Dx è¡¨ç¤ºé‰´
åˆ«å™¨å°†å°½å¯èƒ½åœ°å­¦ä¹ çœŸå®æ ·æœ¬æ•°æ®åˆ†å¸ƒ ï¼›
()Pzz
ä¸ºè¾“å…¥çš„å™ªå£°åˆ†å¸ƒ ï¼›
()Gzè¡¨ç¤ºç”Ÿæˆå™¨ç”Ÿæˆçš„æ ·æœ¬
æ•°æ®åˆ†å¸ƒ ï¼›
( ( ))DG z è¡¨ç¤ºé‰´åˆ«å™¨å°†å°½åŠ›å»é‰´åˆ«ç”Ÿæˆ
å™¨ç”Ÿæˆçš„æ ·æœ¬æ•°æ®æ˜¯å¦ä¸ºçœŸã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…ˆ
å°†ä¸¤è€…ä¹‹ä¸€å‚æ•°å›ºå®šï¼Œä¼˜åŒ–å¦ä¸€æ–¹ï¼Œä¸€æ®µæ—¶é—´å
äº¤æ¢å›ºå®šå’Œä¼˜åŒ–è§’è‰²ã€‚å¦‚æ­¤åå¤åšå¼ˆï¼Œç”Ÿæˆå™¨å’Œ
é‰´åˆ«å™¨æœ€ç»ˆå°†è¾¾åˆ°çº³ä»€å¹³è¡¡ã€‚  
ç†è®ºä¸Šï¼Œ GANç½‘ç»œæœ‰èƒ½å¤Ÿæ‹Ÿåˆä»»ä½•å¤æ‚æ ·æœ¬
æ•°æ®çš„å¯èƒ½æ€§ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ GANåœ¨æ‹Ÿåˆå¤
æ‚æ ·æœ¬æ•°æ®æ—¶è¡¨ç°ä¸å¤Ÿç¨³å®šã€‚å› æ­¤ ï¼ŒGANç½‘ç»œè¢«
æå‡ºã€‚åœ¨ GANç½‘ç»œåŸºç¡€ä¸Šï¼Œ CGANç½‘ç»œé€šè¿‡åŠ 
å…¥æ¡ä»¶çš„æ–¹å¼åŠ å¼ºå¯¹æ•°æ®ç”Ÿæˆçš„å¼•å¯¼ï¼ŒåŠ å…¥çš„æ¡
ä»¶æ˜¯ä»»æ„çš„ï¼Œå¦‚æ ‡ç­¾ä¿¡æ¯ã€æ•°æ®å±æ€§ä¿¡æ¯ç­‰ã€‚åœ¨
CGANç½‘ç»œä¸­ï¼Œç”Ÿæˆç½‘ç»œæ¥æ”¶çš„å°±æ˜¯éšæœºå™ªå£°
z
å’ŒåŠ å…¥çš„æ¡ä»¶
c ã€‚å› æ­¤ï¼Œ CGANçš„ä»£ä»·å‡½æ•°ä¸€èˆ¬ä¸º  
data() )D
(m
)(in max ( ,
Â  log |  l([ ( )] [ ( ( )))] og 1 |)
 =
PG
P E D cG
E G cVD
Dï¾ï¾ +âˆ’
z x x z z xz
ï¼ˆ2ï¼‰ 
é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒåŸæœ¬çš„ GANç½‘ç»œä»æ— ç›‘ç£
æˆ–è€…åŠç›‘ç£çš„å­¦ä¹ è½¬åŒ–æˆæœ‰ç›‘ç£çš„å­¦ä¹ ã€‚ CGAN
ç½‘ç»œå’Œ GANç½‘ç»œä¸€æ ·éƒ½æ˜¯ç›¸äº’äº¤æ›¿è®­ç»ƒã€‚ CGAN
ç½‘ç»œä»è®­ç»ƒé‰´åˆ«ç½‘ç»œå¼€å§‹ï¼Œå†è®­ç»ƒç”Ÿæˆç½‘ç»œã€‚å¯
è§ï¼Œ CGANç½‘ç»œå¯ä»¥ä»æ·»åŠ çš„æ¡ä»¶ä¸­å¾—åˆ°éœ€è¦çš„
ç”Ÿæˆä¿¡æ¯ï¼Œä»è€Œæœ‰åˆ©äºå›¾åƒç”Ÿæˆä»»åŠ¡çš„è¿›è¡Œã€‚  
2  æ‰€æå‡ºçš„ é€†è‰²è°ƒæ˜ å°„ç®—æ³•  
2.1 ç”Ÿæˆç½‘ç»œ  
å—Marnerides ç­‰[5]é‡‡ç”¨å¤šåˆ†æ”¯ç½‘ç»œåˆ†åˆ«æå–
ä½é¢‘ã€ä¸­é¢‘å’Œé«˜é¢‘ç‰¹å¾ç­‰å·¥ ä½œçš„å¯å‘ï¼Œæœ¬æ–‡æ‰€è®¾
è®¡çš„ç”Ÿæˆç½‘ç»œçš„ç»“æ„å¦‚å›¾ 2æ‰€ç¤ºã€‚è¯¥æ¨¡å‹ä½¿ç”¨
256 256ï‚´
å¤§å°çš„ LDRå›¾åƒä½œä¸ºè¾“å…¥ï¼Œ é€šè¿‡ 3ä¸ªåˆ†
æ”¯åï¼Œå¯¹å¾—åˆ° çš„3ä¸ªç‰¹å¾å›¾è¿›è¡Œèåˆå¹¶è¾“å‡ºã€‚åœ¨
layerå±‚ä¸­ï¼Œå·ç§¯æ ¸å°ºå¯¸ä¸º
33ï‚´ ï¼Œè¾¹ç•Œä½¿ç”¨ 1ä½œä¸º
å¡«å……ï¼Œå¹¶ä½¿ç”¨ RELUæ¿€æ´»å‡½æ•°ã€‚ç”±äºä½¿ç”¨ç›®çš„ä¸
åŒï¼Œæ­¥é•¿åœ¨ä¸åŒçš„åˆ†æ”¯ä¸­ç•¥æœ‰ä¸åŒã€‚  
1) åˆ†æ”¯ä¸€ã€‚ä½¿ç”¨äº† 6ä¸ªlayerå±‚å’Œä¸€ä¸ª
44ï‚´ çš„
å·ç§¯å±‚ï¼Œ layerå±‚ä½¿ç”¨çš„å·ç§¯æ ¸å¤§å°ä¸º
33ï‚´ ï¼Œæ­¥é•¿
ä¸º2ï¼Œè¿™æ˜¯ä¸ºäº†å°†è¾“å…¥çš„
256 256ï‚´ å¤§å°çš„å›¾ç‰‡æ±‡
èšä¸º 1ä¸ªåƒç´ ç‚¹ï¼Œä»¥è·å¾—æ•´å¼ å›¾ç‰‡çš„æ„Ÿå—é‡ï¼Œä»
è€Œè·å¾—è‰²è°ƒæ˜ å°„å‰å›¾åƒçš„æ•´ä½“è‰²å½©å˜åŒ–ï¼Œæœ¬åˆ†æ”¯
æœ€ç»ˆè¾“å‡º
1 1 64ï‚´ï‚´ å°ºå¯¸çš„ç‰¹å¾å›¾ã€‚  
2) åˆ†æ”¯äºŒã€‚ä½¿ç”¨ 3ä¸ªlayerå±‚å’Œä¸€ä¸ª
33ï‚´ çš„
å·ç§¯å±‚ï¼Œ layerå±‚æ­¥é•¿æ˜¯ 1ï¼Œä»¥åˆ©äºç”Ÿæˆç½‘ç»œå­¦ä¹ 
åˆ°LDRå›¾åƒçš„ä¸­é¢‘ä¿¡æ¯çš„åˆ†å¸ƒã€‚åœ¨è¯¥ç½‘ç»œåˆ†æ”¯
ä¸­ï¼Œè¾“å…¥å›¾åƒçš„é€šé“æ•°æ˜¯ 3ï¼Œè¾“å‡ºå›¾åƒçš„é€šé“æ•°
æ˜¯64ã€‚ 
3) åˆ†æ”¯ä¸‰ã€‚ä»…ç”± 2ä¸ªlayerå±‚ç»„æˆï¼Œ æ¯ ä¸ªlayer
å±‚éƒ½ä½¿ç”¨
33ï‚´ çš„å·ç§¯æ ¸ï¼Œæ­¥é•¿ä¸º 1ã€‚è¿™ä½¿å¾—è¯¥åˆ†
æ”¯èƒ½å¤Ÿè·å¾—
55ï‚´ çš„æ„Ÿå—é‡ï¼Œä»è€Œèƒ½å¤Ÿè·å¾—å›¾åƒå±€
éƒ¨è¾ƒä¸ºå‰§çƒˆçš„å˜åŒ–ç‰¹å¾ï¼Œè¯¥åˆ†æ”¯æœ€ç»ˆå¾—åˆ°
256 256 128ï‚´ï‚´
å°ºå¯¸çš„ç‰¹å¾å›¾ã€‚  
æ‰€è¾“å…¥çš„ LDRå›¾åƒç»è¿‡ 3ä¸ªåˆ†æ”¯åï¼Œ å…ˆå°†åˆ†
æ”¯ä¸€è·å¾—çš„èƒ½å¤Ÿåæ˜ å…¨å±€è‰²è°ƒå˜åŒ–çš„ä¸€ä¸ªåƒç´ ç‚¹
æ‰©å……ä¸º
256 256ï‚´ å¤§å°ï¼Œè¿™æ ·æ˜¯ä¸ºäº†åœ¨ä¸‹é¢çš„èåˆ
ä¸­ï¼Œä½¿å›¾åƒèƒ½å¤Ÿåæ˜ æ˜ å°„å‰å…¨å±€è‰²è°ƒå˜åŒ–ï¼Œå¹¶ä¸
å¦å¤– 2ä¸ªåˆ†æ”¯ç”Ÿæˆçš„ç‰¹å¾å›¾æŒ‰ç…§ç»´åº¦ä¸²è”æˆ
256 256 256ï‚´ï‚´
å¤§å°ï¼Œæœ€ç»ˆé€šè¿‡ä¸€ä¸ª layerå±‚å°†ç‰¹
å¾å›¾æ•°ç›®å‡å°‘åˆ° 3ï¼Œå¾—åˆ°
256 256 3ï‚´ï‚´ å°ºå¯¸çš„ç”Ÿæˆ4                                                      åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦å­¦æŠ¥   
[é”®å…¥æ–‡å­— ] 
 å›¾åƒï¼Œè¯¥layerå±‚ä¸­ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯ SELUã€‚ 
 
å›¾2  æ‰€æå‡ºçš„ç”Ÿæˆç½‘ç»œç»“æ„  
Fig.2  Structure of the proposed generative network  
2.2 é‰´åˆ«ç½‘ç»œ  
é‰´åˆ«ç½‘ç»œç”¨äºåˆ¤åˆ«ç”Ÿæˆå›¾åƒæ˜¯å¦ç¬¦åˆçœŸå®çš„
æ•°æ®åˆ†å¸ƒã€‚åœ¨æœ¬ æ–‡æ–¹æ³•ä¸­ï¼Œé‰´åˆ«ç½‘ç»œçš„ç»“æ„å¦‚å›¾
3æ‰€ç¤ºã€‚  
      
  
å›¾3  æ‰€æå‡ºçš„é‰´åˆ«ç½‘ç»œåŠå…¶å†…éƒ¨é‰´åˆ«å—  
Fig.3  Structure  of the proposed authentication network and 
its internal authentication block  
å°†ç”Ÿæˆç½‘ç»œç”Ÿæˆçš„å›¾ç‰‡å’ŒåŸ HDRå›¾åƒæŒ‰ç…§
é€šé“æ•°ä¸²è”æˆ
256 256 6ï‚´ï‚´ å¤§å°ï¼Œä½¿ç”¨å›¾ 3æ‰€ç¤ºçš„
é‰´åˆ«å—ï¼ˆ blockï¼‰è¿›è¡Œä¸‹é‡‡æ ·ã€‚å…¶ä¸­ï¼Œæ¯ä¸ªé‰´åˆ«å—
ä½¿ç”¨å·ç§¯æ ¸ä¸º
44ï‚´ ï¼Œæ­¥é•¿ä¸º 2ï¼Œè¾¹ç•Œå¡«å……ä¸º 1çš„
å·ç§¯å±‚ï¼Œå¹¶è·Ÿéšå½’ä¸€åŒ–å¤„ç†ï¼Œä½¿ç”¨ 0.2çš„
LeakyRELU æ¿€æ´»å‡½æ•°ã€‚ å›¾åƒä¾æ¬¡ç»è¿‡ 4ä¸ªé‰´åˆ«å—ï¼Œ
æ¯ç»è¿‡ä¸€ä¸ªé‰´åˆ«å—åç‰¹å¾å›¾éƒ½ä¼šç¼©å° 2å€ï¼Œåœ¨å¾—
åˆ°
32 32 512ï‚´ï‚´ å°ºå¯¸çš„å›¾ç‰‡ä¹‹åï¼Œä½¿ç”¨ä¸€å±‚ä¸é‰´åˆ«
å—ç›¸åŒçš„å·ç§¯å±‚ã€‚é‰´åˆ«å™¨å°†æœ€ç»ˆçš„ç»“æœè¿›è¡Œæ˜¯æˆ–
éçš„åˆ¤åˆ«ã€‚  
2.3 ä»£ä»·å‡½æ•°  
é‰´åˆ«ç½‘ç»œå¯¹çœŸå®æ ·æœ¬çš„åˆ†å¸ƒé€šè¿‡é‰´åˆ«ååˆ¤æ–­
ä¸ºçœŸï¼ˆå³è¾“å‡º 1ï¼‰ï¼Œå¯¹ç”Ÿæˆç½‘ç»œç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œé‰´åˆ«åè¾“å‡ºä¸ºå‡ï¼ˆå³è¾“å‡º 0ï¼‰ã€‚ç”Ÿæˆç½‘ç»œåˆ™å°½åŠ›
ä½¿è‡ªå·±ç”Ÿæˆçš„æ ·æœ¬åˆ†å¸ƒè·å¾—çœŸçš„è¯„ä»·ï¼ˆå³è·å¾—
1ï¼‰ã€‚å› æ­¤ï¼Œç½‘ç»œçš„ä¸€éƒ¨åˆ† æŸå¤±å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºå¼
ï¼ˆ2ï¼‰ã€‚ä¸€æ–¹é¢ï¼Œé‰´åˆ«ç½‘ç»œè¦å¯¹çœŸå®çš„æ ·æœ¬åˆ†å¸ƒç»
è¿‡é‰´åˆ«ç½‘ç»œåç”Ÿæˆçš„åˆ†å¸ƒä¸ç›¸åŒå¤§ å°çš„å…¨ 1çŸ©é˜µ
åšäº¤å‰ç†µï¼Œç”¨äºè¯†åˆ«çœŸå®çš„æ ·æœ¬åˆ†å¸ƒï¼›å¦ä¸€æ–¹é¢ï¼Œ
ç”Ÿæˆç½‘ç»œç”Ÿæˆçš„æ ·æœ¬åˆ†å¸ƒç»è¿‡é‰´åˆ«ç½‘ç»œåï¼Œä¸ç›¸
åŒå¤§å°çš„å…¨ 0çŸ©é˜µåšäº¤å‰ç†µï¼Œç”¨äºä½¿å¾—é‰´åˆ«å™¨å°½
å¯èƒ½åœ°è¯†åˆ«è™šå‡çš„æ•°æ®åˆ†å¸ƒã€‚ä¸ºäº†ä½¿ç”Ÿæˆç½‘ç»œç”Ÿ
æˆçš„æ•°æ®åˆ†å¸ƒèƒ½å¤Ÿå°½åŠ›æ¬ºéª—é‰´åˆ«ç½‘ç»œï¼Œå°†ç”Ÿæˆç½‘
ç»œç”Ÿæˆçš„æ•°æ®åˆ†å¸ƒä¸å…¨ 1çŸ©é˜µåšäº¤å‰ç†µåå†é€å…¥
é‰´åˆ«ç½‘ç»œã€‚æ­¤å¤–ï¼Œåœ¨ç”Ÿæˆç½‘ç»œçš„ æŸå¤±å‡½æ•°ä¸­ï¼Œä½¿
ç”¨å‡æ–¹è¯¯å·® ï¼ˆMean  Square  Errorï¼ŒMSEï¼‰è¿›è¡Œé€
åƒç´ çš„å·®åˆ«è®¡ç®—ï¼Œä»¥ä¿è¯ç”Ÿæˆå›¾åƒå’ŒçœŸå®æ ·æœ¬å›¾
åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚å…¶ä¸­ï¼Œ MSEçš„è¡¨è¾¾å¦‚ä¸‹ï¼š  
2
11
MSE ( ) ( )
2n
ii
iy x a x
n==âˆ’ïƒ¥
 ï¼ˆ3ï¼‰ 
å¼ä¸­ï¼š
y ä¸ºçœŸå®çš„æ•°æ®åˆ†å¸ƒ ï¼›
aä¸ºç”Ÿæˆç½‘ç»œç”Ÿæˆ
çš„æ•°æ®åˆ†å¸ƒ ï¼›
ixä¸ºå¯¹åº”çš„åƒç´ åˆ†å¸ƒã€‚  
æœ€ç»ˆï¼Œæ•´ä¸ªç½‘ç»œçš„ æŸå¤±å‡½æ•°ä¸º  
DLoss min max ( , ) MSE
GV G D =+
 ï¼ˆ4ï¼‰ 
3  å®éªŒç»“æœä¸åˆ†æ  
3.1 å®éªŒè®¾ç½®  
é€šè¿‡ä»æµ·é‡è§†é¢‘ä¸­æ”¶é›†äº† 8262å¼ HDRé«˜æ¸…
å›¾åƒï¼Œåœ¨é€ä¸€å‰”é™¤ä¸è‰¯æ•°æ®ä¹‹åï¼Œé€‰å–äº† 6736
å¼ å¤§å›¾åƒï¼Œ å†å°†è¿™äº›å¤§å›¾åƒé€ä¸€å‰ªè£æˆ 30312å¼ 
å°ºå¯¸ä¸º
960 480ï‚´ çš„å°å›¾åƒã€‚åœ¨å‰”é™¤ç›¸ä¼¼åœºæ™¯åï¼Œ
æœ€ç»ˆå¾—åˆ° 7632å¼ 
960 480ï‚´ å¤§å°çš„å›¾åƒã€‚ æœ¬å®éªŒé€‰
å–å…¶ä¸­ 4000å¼ å›¾åƒï¼Œå¹¶éšæœºå‰ªè£æˆ
256 256ï‚´ å¤§
å°ï¼Œç”¨æ¥ä½œä¸ºæ ‡ç­¾æ•°æ®ã€‚å¯¹äºè¿™äº›å›¾åƒï¼Œä½¿ç”¨
OpenCVä¸­çš„è‰²è°ƒæ˜ å°„ ç®—æ³•éšæœºè¿›è¡Œæ˜ å°„ï¼Œå¾—åˆ°
çš„å›¾åƒè¢«ä½œä¸ºå¯¹åº”çš„ LDRå›¾åƒæ ·æœ¬ã€‚ é™¤ä»¥ä¸Š 4000
å¼ è®­ç»ƒå›¾åƒå¤–ï¼Œ ä»è·å– çš„7632å¼ å›¾åƒä¸­å»æ‰è®­ç»ƒ
é›†åï¼Œéšæœºé€‰å–ä¸é‡å¤çš„ 20å¼ å›¾åƒä½œä¸ºæµ‹è¯•é›†ï¼Œ
æµ‹è¯•é›†çš„ç¼©ç•¥å›¾å¦‚å›¾ 4æ‰€ç¤ºã€‚                                               è´æ‚¦ç­‰ï¼š åŸºäºæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ HDRå›¾åƒç”Ÿæˆæ–¹æ³•                                 
5 
 
 
 
å›¾4  å®éªŒæ‰€ä½¿ç”¨çš„ 20å¼ LDRæµ‹è¯•å›¾ç‰‡  
Fig.4  20 LDR test pictures used in this experiment  
æœ¬å®éªŒä½¿ç”¨ Adamä¼˜åŒ–å™¨ï¼Œç”Ÿæˆç½‘ç»œçš„å­¦ä¹ 
ç‡è®¾ä¸º
-310 ï¼Œé‰´åˆ«ç½‘ç»œçš„å­¦ä¹ ç‡è®¾ ä¸º
-45 10ï‚´ ã€‚åœ¨
è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œå³ä½¿å·²ç»å°†ç”Ÿæˆç½‘ç»œçš„å­¦ä¹ ç‡è®¾
ç½®æˆé‰´åˆ«ç½‘ç»œçš„ 2å€ï¼Œåˆ¤åˆ«ç½‘ç»œ ä¹Ÿæ€»èƒ½è½»è€Œæ˜“ä¸¾
åœ°åœ¨å¯¹æŠ—ä¸­å–å¾—ä¸Šé£ï¼Œè¿™æ ·å°±ä¼šå¯¼è‡´ç”Ÿæˆç½‘ç»œçš„
æŸå¤±å‡½æ•°å‡ ä¹æ²¡æœ‰ä¸‹é™ï¼Œäº§ç”Ÿ GANç½‘ç»œå¸¸è§çš„
æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œ æŸå¤±å‡½æ•°å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚
å› æ­¤ï¼Œè®­ç»ƒä¸­ï¼Œåœ¨æ›´æ–°
k æ¬¡ç”Ÿæˆç½‘ç»œå‚æ•°çš„åŒæ—¶
åªæ›´æ–°ä¸€æ¬¡é‰´åˆ«ç½‘ç»œçš„å‚æ•°ï¼Œä»¥ä¿è¯æ§åˆ¶é‰´åˆ«ç½‘
ç»œçš„é‰´åˆ«èƒ½åŠ›ä¸ç”Ÿæˆç½‘ç»œçš„ç”Ÿæˆèƒ½åŠ›å°½é‡æŒå¹³ã€‚
å¦‚æœ
kå€¼è¿‡å¤§ï¼Œå°±ä¼šè®©ç”Ÿæˆç½‘ç»œçš„ æŸå¤±å‡½æ•°çš„å€¼
æ¥å›éœ‡è¡ ; kå€¼è¿‡å°ï¼Œé‰´åˆ«ç½‘ç»œ ä¼šè¾¾åˆ°å±€éƒ¨æœ€ä¼˜
ç‚¹ï¼Œäº§ç”Ÿæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚åœ¨å®éªŒä¸­ï¼Œæ ¹æ®è®­ç»ƒé›†
å¤§å°ï¼Œè®©ç”Ÿæˆç½‘ç»œæ›´æ–° 5æ¬¡ï¼Œé‰´åˆ«ç½‘ç»œæ›´æ–° 1æ¬¡ã€‚
å¦‚æ­¤ï¼Œæ¨¡å‹èƒ½å¤Ÿè¾ƒå¿«æ”¶æ•›ä¸”é‡å»ºå›¾åƒçš„è´¨é‡è¾ƒä¸º
ç¨³å®šã€‚  
å®éªŒåŸºäº Pytorchå¹³å°å±•å¼€ï¼Œä½¿ç”¨ CPU 
i9-9900K CPU@3.60GHz å¤„ç†å™¨ï¼Œ 32GB è¿è¡Œå†…
å­˜ï¼Œ NVIDIA GeForce RTX 2080Ti GPU è¿›è¡Œè®­ç»ƒ
ä¸æµ‹è¯•ã€‚ç½‘ç»œè®­ç»ƒæ—¶é—´çº¦ä¸º 40hã€‚ 
3.2 å®éªŒç»“æœ  
ä½¿ç”¨æ‰€å¾—åˆ°çš„æ¨¡å‹å¯¹ å›¾4æ‰€ç¤ºçš„ 20å¼ å›¾åƒè¿›
è¡Œæµ‹è¯•ï¼Œå¹¶ç»™å‡ºäº†å®¢è§‚ä¸ä¸»è§‚æµ‹è¯•ç»“æœã€‚  
ä½¿ç”¨ PSNRã€MPSNRã€SSIMã€MS-SSIMåŠ
HDR -VDPå…±5ä¸ªå®¢è§‚æŒ‡æ ‡å¯¹ HDRé‡å»ºå›¾åƒçš„è´¨
é‡è¿›è¡Œè¯„ä»·ã€‚å…¶ä¸­ï¼Œ MS-SSIMå®¢è§‚åæ˜ äº†å›¾åƒåœ¨
å¤šå°ºåº¦ä¸Šçš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œ HDR -VDPç”¨ç®—æ³•æ¥æ¨¡
æ‹Ÿäººçœ¼è§‚çœ‹å›¾åƒçš„è¿‡ç¨‹ã€‚ MS-SSIMä¸HDR -VDP
æ˜¯ç”¨æ¥è¯„ä»· HDRé‡å»ºå›¾åƒè´¨é‡çš„é‡è¦æŒ‡æ ‡ã€‚ä½¿
ç”¨ä¸åŒçš„æ–¹æ³•å¯¹å›¾  4æ‰€ç¤ºçš„æµ‹è¯•é›†è¿›è¡Œæµ‹è¯•ï¼Œæ‰€
å¾—åˆ°çš„å¹³å‡å€¼å¦‚è¡¨ 1æ‰€ç¤ºã€‚å¯ä»¥çœ‹å‡ºï¼Œä¸ä»¥å¾€æ–¹æ³•DrTMO[2]ã€ExpandNet[5]ä¸HDRCNN[3]ç›¸æ¯”ï¼Œ
æœ¬æ–‡æ–¹æ³•è·å¾—äº†æœ€é«˜ SSIMã€MS-SSIMä¸
HDR -VDPå€¼ï¼Œåæ˜ äº†æ‰€ææ–¹æ³•èƒ½å¤Ÿè¾ƒå¥½åœ°é‡å»º
å‡ºHDRå›¾åƒã€‚  
æ­¤å¤–ï¼Œè¿˜å¯¹æ¯”äº†ä¸åŒæ–¹æ³•æ‰€å¾—åˆ°çš„ HDRå›¾
åƒçš„ä¸»è§‚è´¨é‡ã€‚ç”±äºæ™®é€šæ˜¾ç¤ºå™¨æ— æ³•æ˜¾ç¤º HDR
å›¾åƒï¼Œå°†æ‰€æœ‰æ–¹æ³•å¾—åˆ°çš„ HDRå›¾åƒéƒ½é€šè¿‡åŒæ ·
çš„æ–¹æ³•å†æ˜ å°„ä¸º LDRå›¾åƒï¼Œ ä»¥ä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨æ™®é€š
æ˜¾ç¤ºå™¨ä¸Šå¯è§†åŒ–ã€‚ å¦‚å›¾ 5æ‰€ç¤ºï¼ŒDrTMO[2]æ–¹æ³•ç”Ÿ
æˆçš„å›¾åƒåç™½ï¼Œä¸åŸ HDRå›¾åƒå·®è·è¾ƒå¤§ï¼›
ExpandNet[5]åœ¨æœ‰äº›å›¾åƒä¸Šæœ‰ä¸¥é‡çš„è‰²åº¦å¤±çœŸï¼Œ å½±
å“äº†ä¸»è§‚è´¨é‡ï¼› HDRCNN[3]å¯¹äºæ›å…‰åŒºåŸŸä½äº 5%
çš„å›¾åƒæœ‰è¾ƒå¥½æ•ˆæœï¼Œä½†æ˜¯ä¹Ÿä¹Ÿä¼šäº§ç”Ÿè‰²åº¦åç§»ï¼Œ
å¦‚ç¬¬äºŒè¡Œå³ä¸Šè§’çš„å¤©ç©ºæœ‰æ˜æ˜¾çš„è‰²åº¦åç§»ã€‚ä¸ä¸Š
è¿°æ–¹æ³•ç›¸æ¯”ï¼Œ æœ¬æ–‡æ–¹æ³•å¯ä»¥å¾—åˆ°è´¨é‡è¾ƒé«˜çš„ HDR
å›¾åƒï¼Œå›¾åƒçœ‹ä¸Šå»è‡ªç„¶ï¼Œä¸åŸ HDRå›¾åƒè¾ƒä¸ºæ¥
è¿‘ã€‚ExpandNet[5]æ–¹æ³•ä¸­å‡ºç°çš„è‰²å½©åç§»ç­‰ç°è±¡ä¹Ÿ
å¾—åˆ°äº†è§£å†³ã€‚ æ­¤å¤–ï¼Œæ‰€è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ä¸ä»…ä»…é’ˆ
å¯¹æŸä¸€ç±»åœºæ™¯ï¼Œåœ¨å„ç§åœºæ™¯ï¼Œå„ç§äº®åº¦æ¡ä»¶ä¸‹éƒ½
èƒ½å¾—åˆ°è¾ƒå¥½è´¨é‡çš„é‡å»ºå›¾åƒã€‚ å¦‚ å›¾6æ‰€ç¤ºï¼Œ åœ¨ LDR
å›¾åƒçš„ BT.709è‰²åŸŸéš¾ä»¥æ˜¾ç¤ºçš„æ¨¡ç³Šç‰¹å¾ä¹Ÿèƒ½å¤Ÿ
åœ¨æ‰€å¾—åˆ°çš„ HDRå›¾åƒä¸­æœ‰æ›´åŠ æ¸…æ™°çš„è¡¨è¾¾ï¼›å¦‚
å›¾7æ‰€ç¤ºï¼Œåœ¨æ˜äº®åœºæ™¯ä¸‹é‡å»º çš„å›¾åƒè‰²å½©è‡ªç„¶ï¼Œ
æ¨¡ç³ŠåŒºåŸŸä¹Ÿæœ‰ä¸€å®šç¨‹åº¦çš„äº®åº¦æå‡ï¼Œå¹¶ä¸”æ²¡æœ‰æ˜
æ˜¾çš„è‰²å½©åç§»ç°è±¡ã€‚  
4  æ¶ˆèå®éªŒ  
ä¸ºäº†éªŒè¯å¤šåˆ†æ”¯ç½‘ç»œçš„æœ‰æ•ˆæ€§ï¼Œå¯¹ç”Ÿæˆç½‘ç»œ
è¿›è¡Œäº†æ¶ˆèç ”ç©¶ã€‚  
é¦–å…ˆï¼Œå»æ‰äº†æå–ä¸­é¢‘ä¿¡æ¯çš„åˆ†æ”¯ï¼Œ æ‰€å¾—å®
éªŒç»“æœå¦‚è¡¨ 2æ‰€ç¤ºã€‚å¯ä»¥çœ‹å‡ºï¼Œå»æ‰ä¸­é¢‘åˆ†æ”¯ å
çš„ç”Ÿæˆç½‘ç»œæ‰€è·å¾—çš„å®¢è§‚æŒ‡æ ‡æ•°å€¼å‡ä½äºæ‰€æå‡º
çš„ä¸‰åˆ†æ”¯ç½‘ç»œ ï¼Œè·å¾—çš„ä¸»è§‚å›¾åƒè´¨é‡çš„è‰²å½©é¥±å’Œ
åº¦ä¸è¶³ï¼Œäº®åº¦ä¸å¤Ÿé«˜ï¼Œå¦‚ å›¾8ï¼ˆaï¼‰æ‰€ç¤ºã€‚ 
ç„¶åï¼Œä¸ºè¿›ä¸€æ­¥éªŒè¯å¤šåˆ†æ”¯ç½‘ç»œçš„æœ‰æ•ˆæ€§ï¼Œ
åˆ†åˆ«å¯¹ 3ä¸ªåˆ†æ”¯è¿›è¡Œäº†å•ç‹¬è®­ç»ƒï¼Œå®éªŒ ç»“æœå¦‚è¡¨
2æ‰€ç¤ºã€‚å›¾ 8å±•ç¤ºäº†ä¿ç•™ä¸åŒåˆ†æ”¯åçš„ä¸»è§‚ç»“æœ ï¼Œ
å¯ä»¥çœ‹å‡º ï¼Œå•ç‹¬åˆ†æ”¯ æ‰€ç”Ÿæˆçš„å›¾åƒ å­˜åœ¨æ˜æ˜¾çš„ å
è‰²ï¼Œä¸”çº¹ç†ç»†èŠ‚ ä¸å¤Ÿä¸°å¯Œ ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ ç”±äº
ä½é¢‘åˆ†æ”¯ ä¼šå°†è¾“å…¥çš„
256 256ï‚´ å¤§å°çš„å›¾åƒæ±‡èšåˆ°
1ä¸ªåƒç´ ç‚¹ï¼Œä»¥è·å¾—æ•´å¼ å›¾ç‰‡çš„æ„Ÿå—é‡ï¼Œ ä½é¢‘åˆ†
æ”¯æœ€ç»ˆçš„è¾“å‡ºå°ºå¯¸æ˜¯
11ï‚´ ã€‚ä¸ºç›´è§‚ åœ°å±•ç¤ºä½é¢‘åˆ†
æ”¯çš„è¾“å‡ºå›¾åƒï¼Œ å°†å…¶æ”¾å¤§ä¸º
256 256ï‚´ å¤§å°çš„å›¾åƒ ï¼Œ
å¦‚å›¾ 8ï¼ˆdï¼‰æ‰€ç¤ºã€‚ 4ç§æ¶ˆèå®éªŒè¯æ˜ï¼Œ æœ¬æ–‡æ‰€æ6                                                      åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦å­¦æŠ¥   
[é”®å…¥æ–‡å­— ] 
 å‡ºçš„ç”Ÿæˆç½‘ç»œ èƒ½å¤Ÿæ˜¾è‘— æé«˜å›¾åƒçš„è´¨é‡ï¼Œä½¿å¾—ç”Ÿ
æˆå›¾åƒè‰²å½©é¥±å’Œåº¦æ›´é«˜ï¼Œä¸»è§‚è´¨é‡æ›´å¥½ï¼Œæ›´åŠ æ¥
è¿‘å‚è€ƒå›¾åƒã€‚  
æ­¤å¤–ï¼Œå°† 3ä¸ªåˆ†æ”¯è¾“å‡ºçš„ç‰¹å¾å›¾è¿›è¡Œäº†å¯è§†
åŒ–ï¼Œç»“æœå¦‚å›¾ 9æ‰€ç¤ºã€‚  
5  ç»“ è®º 
åŸºäº GANç½‘ç»œæ¶æ„ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ LDR
å›¾åƒåˆ° HDRå›¾åƒçš„é€†è‰²è°ƒæ˜ å°„æ–¹æ³•ã€‚ ï¼ˆ1ï¼‰ä¸ºæ­¤ï¼Œ
æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„ç”Ÿæˆç½‘ç»œï¼Œ ä½¿ç”¨å¤šåˆ†æ”¯ç»“æ„ï¼Œæå–å›¾åƒ ä½é¢‘ã€ä¸­é¢‘ã€ é«˜é¢‘ç‰¹å¾ï¼Œå¹¶å°†ä¸åŒå°ºåº¦
çš„ç‰¹å¾è¿›è¡Œèåˆå¾—åˆ°æœ€ç»ˆçš„ HDRå›¾åƒï¼›ï¼ˆ2ï¼‰è®¾
è®¡äº†ä¸€ç§æ–°çš„é‰´åˆ«å™¨ç½‘ç»œï¼Œä½¿ç”¨ç»„åˆçš„é‰´åˆ«å—å®Œ
æˆé‰´åˆ«ä»»åŠ¡ã€‚ ï¼ˆ3ï¼‰å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•
åœ¨å®¢è§‚æŒ‡æ ‡ä¸Šè¶…è¿‡äº†ä»¥å¾€çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨ç›´æ¥
åæ˜  HDRé‡å»ºå›¾åƒè´¨é‡çš„ MS-SSIMä¸
HDR -VDPæŒ‡æ ‡ä¸Šå–å¾—äº†è¾ƒé«˜æ€§èƒ½ã€‚åœ¨ä¸»è§‚è´¨é‡
ä¸Šï¼Œæ‰€é‡å»ºçš„ HDRå›¾åƒä¸»è§‚è´¨é‡è‡ªç„¶å¹¶ä¸”æ— æ˜¾
è‘—è‰²å½©å¤±çœŸã€‚  
 
LDRå›¾åƒ HDRå›¾åƒ DrTMO ExpandNet HDRCNN æ‰€æå‡ºçš„æ–¹æ³•
å›¾ 5  ä¸åŒæ–¹æ³•å¾—åˆ°çš„ HDRå›¾åƒçš„ä¸»è§‚æ•ˆæœå¯¹æ¯”  
Fig.5  Comparison of subjective effects of HDR images obtained by different methods  
è¡¨ 1 ä¸åŒæ–¹æ³•çš„å®¢è§‚æ€§èƒ½æ¯”è¾ƒ  
Table 1 Comparison of objective performance among  different methods  
æ–¹æ³•  PSNR  MPSNR  SSIM  MS-SSIM  HDR -VDP -2 
DrTMO[2] 22.31  22.44  0.58 0.59 63.79  
ExpandNet[5]  23.61  23.79  0.70 0.71 78.02  
HDRCNN[3] 25.70  25.95  0.60 0.63 70.93  
æ‰€æå‡ºçš„æ–¹æ³•  24.99  25.26  0.71 0.77 78.67  
                                              è´æ‚¦ç­‰ï¼š åŸºäºæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ HDRå›¾åƒç”Ÿæˆæ–¹æ³•                                 
7 
 
 è¡¨ 2 æ¶ˆèå®éªŒï¼šç½‘ç»œä¸­ä¸åŒåˆ†æ”¯ çš„å®¢è§‚æ€§èƒ½æ¯”è¾ƒ  
Table 2 Comparison of objective performance of different branches of network  in ablation  experiment  
æ–¹æ³•  PSNR  MPSNR  SSIM  MS-SSIM  HDR -VDP -2 
æ‰€æå‡ºçš„æ–¹æ³•  24.99  25.26  0.71 0.77 78.67  
å»æ‰ä¸­é¢‘åˆ†æ”¯  22.06  22.46 0.61 0.65 77.73  
ä»…ä¸­é¢‘åˆ†æ”¯  22.36  30.46  0.42 0.45 77.54  
ä»…é«˜é¢‘åˆ†æ”¯  22.84  42.44  0.45 0.47 78.72  
               
  
å›¾6  ä½è‰²åŸŸæ¨¡ç³Šåœºæ™¯ä¸‹çš„ HDRå›¾åƒé‡å»º                   å›¾7  ä¸€èˆ¬åœºæ™¯ä¸‹çš„ HDRå›¾åƒé‡å»º  
(å·¦ï¼š LDRï¼Œå³ï¼šé‡å»º HDR )                           (å·¦ï¼š LDRï¼Œå³ï¼šé‡å»º HDR ) 
Fig.6  HDR image reconstruction in low -color -gamut blurred scene ã€‚ Fig.7  HDR  image reconstruction in general scenes  
(Left: LDR, Right: Reconstructed HDR)                         (Left: LDR, Right: Reconstructed HDR)  
 
å›¾8  æ¶ˆèå®éªŒï¼šä¿ç•™ç½‘ç»œä¸­ä¸åŒåˆ†æ”¯ æ‰€å¾—åˆ°çš„ä¸»è§‚å›¾åƒè´¨é‡  
ï¼ˆå·¦ 1ï¼šå»æ‰ä¸­é¢‘åˆ†æ”¯ï¼Œå·¦ 2ï¼šä»…ä¸­é¢‘åˆ†æ”¯ï¼Œå·¦ 3ï¼šä»…é«˜é¢‘åˆ†æ”¯ ï¼Œå·¦ 4ï¼šä»…ä½é¢‘åˆ†æ”¯ï¼Œå·¦ 5ï¼šæ‰€æå‡ºæ–¹æ³• :ï¼‰ 
Fig.8  Subjective image quality by retaining different branches of network  in ablation  experiment  
ï¼ˆLeft 1: Remove the middle -frequen cy branch , Left 2: Only middle-frequenc y branch , Left 3: Only high -frequenc y branch ,  
Left 4: Only low-frequenc y branch,  Left 5: Proposed method ï¼‰ 
 
å›¾9  ä¸åŒåˆ†æ”¯è¾“å‡ºçš„ç‰¹å¾å›¾ï¼ˆå·¦ï¼š ä½é¢‘åˆ†æ”¯ ï¼Œä¸­ï¼šä¸­é¢‘åˆ†æ”¯ ï¼Œå³ï¼šé«˜é¢‘åˆ†æ”¯ ï¼‰ 
Fig.9  Feature map s output from  different branch es 
 (left: low-frequenc y branch, middle: middle -frequenc y branch , right: high-frequenc y branch ) 
 
  
 8                                                      åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦å­¦æŠ¥   
 
 å‚è€ƒæ–‡çŒ®ï¼ˆ References ï¼‰ 
 
[1] é©¬æ­£å…ˆ . HDRæŠ€æœ¯åŠå…¶åœ¨ 4Kè¶…é«˜æ¸…ç”µè§†ä¸Šçš„åº”ç”¨ [J]. ç”µè§†
æŠ€æœ¯ , 2019, 43(1):33 -39.               
MA Z X.HDR technology  and application  on 4K 
ultra-high-definition  TV[J].Television Technology ,2019, 
43(1):33 -39(in  Chinese).  
[2] ENDO  Y ,KANAMORI Y ,MITANI J.Deep reverse tone 
mapping[J ].ACM Transactions on 
Graphics, 2017,36(6):177:1 -177:10.   
[3] EILERTSEN G,KRONANDER J,DENES G,et al.HDR image 
reconstruction from a single exposure using deep 
CNNs[J ].ACM Transactions on Graphics,  2017,36(6):1 -15. 
[4] XU Y C,SONG  L,XIE R, et al. Deep video inverse tone 
mapping[C]//2019 IEE E Fifth International Conference on 
Multimedia Big Data (BigMM). Piscataway :IEEE Press, 
2019:142 -147. 
[5] MARNERIDES  D,BASHFORD â€ROGERS  T,HA TCHETT  J, 
et al.ExpandNet:A  deep convolutional neural network for high 
dynamic range expansion from low dynamic range 
content[C]//Computer Graphics Forum ,2018,37(2):37 -49.  
[6] KINOSHITA Y ,KIY A H.iTM -Net: Deep inverse tone mapping 
using novel loss function considering tone mapping 
operator[J].IEEE Access,2019,7:73555 -73563.   
[7] LEE S,AN G H,KANG S J.Deep chain HDRI: Reconstructing 
a high dynamic range image from a single low dynamic range 
image[J ].IEEE Access,2018,6:49913 -49924.  
[8] XU Y C,NING S Y,XIE R,et al.Gan based multi -exposure 
inverse t one mappin g[C]//2019 IEEE International 
Conference on Image Processing (ICIP). Piscataway :IEEE 
Press,2019:1 -5.  
[9] NING S Y,XU H T,SONG L,et al.Learning an inverse tone 
mapping network with a generative adversarial 
regularizer[C]//2018 IEEE International Confe rence on 
Acoustics, Speech and Signal Processing (ICASSP). 
Piscataway :IEEE Press,2018:1383 -1387.   
[10] LEE S,AN G H,KANG S J.Deep recursive HDRI : Inverse 
tone mapping using generative adversarial 
networks[C]//Proceedings of the European Conference on 
Computer V ision (ECCV).  Berlin :Springer,2018:596 -611.  
[11] RONNEBERGER O,FISCHER P,BROX T.U -net: 
Convolutional networks for biomedical image 
segmentation[C]//International Conference on Medical image 
Compu ting and Computer -Assisted Inter vention.  
Berlin :Springer, 2015:23 4-241. 
[12] TAKEUCHI M,SAKAMOTO Y ,YOKOY AMA R,et al.A 
Gamut -extension method considering color information 
restoration using convolutional neural net works[C]//2019 IEEE International Conference on Image Processing 
(ICIP). Piscataway :IEEE Press,2019:774 -778. 
[13] LEDIG  C,THEIS L,HUSZÃR F,et al.Photo -realistic single 
image super -resolution using a generative adversarial 
network[C]//Proceedings of the IEEE Confe rence on 
Computer Visi on and Pattern Recog nition. Piscataway :IEEE 
Press,2017:4681 -4690.  
[14] W ANG X T,KE Y,WU S X,et al.EsrGAN:Enhanced 
super -resolution generative adversarial 
networks[C]//Proceedings of the European Conference on 
Computer Vision (ECCV). Berlin :Springer,2018:0 -0. 
[15] ISOLA P,ZHU J Y ,ZHOU T,et al.Image -to-image translation 
with conditional adversarial netwo rks[C]//Proceedings of the 
IEEE Confere nce on Computer Vi sion and Pattern 
Recogni tion.Piscataway :IEEE  Press ,2017:1125 -1134.  
[16] ZHU J Y ,PARK T,ISOLA P,et al.Unpaired image -to-image 
translation using cycle -consistent adversarial 
networks[C]//Proceedings of the IEEE International 
Conferenc e on Computer Visi on.Piscataway :IEEE 
Press,2017:2223 -2232.  
[17] SIAROHIN A,SANGINETO E,LA THUILIÃˆRE S,et 
al.Deformable GANs for pose -based human image 
generation[C]//Proceedings of the IEEE Conference on 
Computer Vision and Pattern Re cognition. Piscataway : IEEE 
Press,2018:3408 -3416.  
[18] CHAN C,GINOSAR S,ZHOU T H,et al.Everybody dance 
now[C]//Proceedings of the IEEE International Conference on 
Computer Vision. Piscataway :IEEE Press,2019:5933 -5942.  
[19] GOODFELLOW I,POUGET -ABADIE J,MIRZA M,et al. 
Generative adversarial nets[C]//Advances in Neural 
Information Processing System sï¼Œ2014:2672 -2680.  
[20] RA TLIFF L J,BURDEN S A,SASTRY S S.Characterization 
and computation of local Nash  equilibria in continuous 
games[C]//2013 51st Annual Allerton Conference on 
Com munication,Control,and Computing. Piscataway :IEEE 
Press,2013:917 -924. 
 
 
 
 
 
 
 
 
 
 
 
HDR image generation method based on conditional  generative advers arial                                              è´æ‚¦ç­‰ï¼š åŸºäºæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ HDRå›¾åƒç”Ÿæˆæ–¹æ³•                                 
9 
 
 network  
BEI Y ue1, Wang Qi1, CHENG Zhipeng1, PANG Xinghao1, YANG  Mohan2, DING Dandan2* 
ï¼ˆ1. MIGU Video Co., Ltd.,  Shanghai 201201,  Chinaï¼› 
2. Beijing Bravo Video Technologies Incorporation , Beijing 100 036, Chinaï¼‰ 
Abstract : Compared with Low Dynamic Range (LDR) images, High Dynamic Range (HDR) images have 
a wider color gamut and higher brightness range, which is more in line with human visual effects. However, 
since most of the current image acquisition devices are LDR device s, HDR image resources are scarce. An 
effective way to solve this problem is to map LDR images to HDR images through inverse tone mapping. This 
paper proposes an inverse tone mapping algorithm based on Conditional Generative Adversarial Network 
(CGAN) to r econstruct HDR images. To this end, a multi -branch -based generation network and a discrimination  
network based on discrimination  blocks are designed, and the data generation and feature extraction capabilities 
of CGAN are used to map a single LDR image fro m the BT.709 color gamut to the corresponding BT.2020 
color area. The e xperimental results show that the proposed network can obtain higher objective and subjective 
quality compared with the existing methods. Especially for fuzzy areas in the low color gam ut, the proposed 
method can reconstruct clearer textures and details.  
 
 Keywords : conditional  generative adversarial network; convolutional neural network; inverse tone 
mapping; gamut mapping; feature extraction  
 
 
 
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”  
Received: 2020 -09-14; Accepted: 2021-04-23; Published online: 2021 -05-17 
URL:  https://kns.cnki.net/kcms/detail/11.2625.v.20210517.1523.003.html  
Foundation item: Zhejiang Provincial Natural Science Foundation of China ( LY20F010013 ) 
*Corresponding author.  E-mail: DandanDing@hznu.edu.cn  
 "
æ°´ä¸‹å›¾åƒè‹±æ–‡è®ºæ–‡-å•å¼ å‡¯-é™ˆé¾™An Efficient Learning-based Framework for Underwater Image Enhancement.pdf,D:/tool/Pycharm/summerProject/examples\æ°´ä¸‹å›¾åƒè‹±æ–‡è®ºæ–‡-å•å¼ å‡¯-é™ˆé¾™An Efficient Learning-based Framework for Underwater Image Enhancement.pdf,"A Efï¬cient Learning-based Framework for
Underwater Image Enhancement
ZhangKai Lv, Long Chen, and Dandan Ding
School of Information Science and Engineering, Hangzhou Normal University
Hangzhou, China, 311121
Email: DandanDing@hznu.edu.cn
Abstract â€”Underwater image enhancement, which targets re-
moving the noise, scattering effect, as well as the bluish,
greenish, or yellowish tone from the underwater images, has
attracted much attention. Earlier rule-based methods generally
assume some prior knowledge when performing constrained
optimization. Recently, the learning-based methods have exhib-
ited superior performance over the rule-based methods. This
paper proposes a simple yet effective convolution neural network
(CNN)-based framework to enhance underwater images. Our
method consists of two stages, CNN-based enhancement and
YUV-based post-processing. In the ï¬rst stage, a lightweight CNN
network is developed to extract the latent features from the
input images for enhancement. Speciï¬cally, our CNN cascades
three residual groups which utilize channel attention blocks
to strengthen the feature extraction capability. In the second
stage, the output of the CNN model is transformed to the YUV
color space where the luminance component is further corrected,
improving the brightness of the whole image. Our model is
trained using the UIEB data set and tested on 100 underwater
images. Experimental results show that our method outperforms
state-of-the-art methods both qualitatively and quantitatively.
Moreover, in terms of computational complexity, our CNN model
costs 437k parameters, which is only 39.5% and 22.5% of the
existing CNN-based methods WaterNet and UWGAN.
Index Terms â€”CNN, Underwater image enhancement, Channel
attention, Color space
I. I NTRODUCTION
With the widespread aggravation of land resource shortage,
population expansion, and environmental degradation, which
pose potential hazards to the oceanâ€”â€”the crucial core of the
life-support system and the development of human society.
Consequently, various coastal countries have aroused their
environmental awareness and accelerated the research, devel-
opment, and utilization of the ocean. When exploiting marine
resources, the vision system is deemed as one of the most
effective detection methods for underwater robots. However,
the complexity of imaging in water and the difference of
law of light propagation cause inevitable degradation to the
underwater images such as color cast, low light, and low
contrast. Thus, underwater image enhancement with the aim
of restoring visually pleasing and clear images has become a
key issue in marine missions.
Earlier underwater image enhancement methods are rule-
based . The non-physical methods, such as histogram equal-
ization [1], auto white balance and fusion these two methods
to enhance underwater images [2], directly adjust the dynamic
value range and enhance the contrast by changing the grayvalue of the pixels in either spatial or frequency domain, with-
out considering the physical procedure of underwater degrada-
tion. These methods could remove certain noise and enhance
edge-details, whereas their performance is circumscribed due
to the physical characteristics of underwater imaging are not
taken into account.
Physical methods resolve such problems by abstracting and
modelling the underwater degradation process of imaging.
As such, undegraded images are accessible through inverting
the degradation process. For instance, B.L. McGlamery [3]
proposed the calculation model of underwater imaging system;
Jules S. Jaffe [4] optimized an underwater imaging system;
based on the imaging model, Nicholas c [5] proposed a
method through eliminating light scattering in underwater
images; He [6] proposed a single-image defogging method
based on dark channel prior (DCP). Generally, these physical
methods gain more superiority than the non-physical ones.
However, they are incapable of removing noise and blurring
artifacts from the underwater images, causing restrictions on
the quality of the processed images. To address this issue,
researchers introduced restoration techniques to obtain more
realistic underwater images. Lu [7] proposed the color cor-
rection method based on spectral characteristics to restore
distorted colors; Peng [8] proposed a depth estimation method
based on blur and absorption (IBLA); Song [9] proposed a
depth estimation method combined underwater light attenua-
tion prior with linear regression model to estimate background
light and transmitted light in RGB color space. Nevertheless,
these methods of image enhancement are usually based on the
premise of obtaining prior knowledge and information, thus
can only be managed under speciï¬c scenarios.
Beneï¬ting from the development of neural networks,
learning-based methods gain extensive implementation in
image defogging, deblurring, and restoration, etc. Research
demonstrates that Convolutional Neural Network (CNN) could
exert better performance than the traditional methods both
objectively and subjectively. Inspired by the previous work,
researchers introduced CNN to improve the quality enhance-
ment of underwater images. Chen [10] proposed a scheme
combined a ï¬ltering-based with a GAN-based restoration.
Li [11] designed an end-to-end WaterGAN consisting of
a depth estimation module followed by a color correction
module. Wang [12] proposed UWGAN learning the nonlinear
mapping between undistorted and distorted images. All abovemethods generate enhanced results by utilizing end-to-end
and data-driven training mechanisms and obtaining better
performance than rule-based methods. On the other hand,
these CNN models are computationally expensive because
they generally cost millions of parameters, which challenges
practical applications.
Instead, this paper systematically proposes an efï¬cient,
low complexity framework for underwater image enhance-
ment. Our framework consists of two stages, CNN-based
enhancement and YUV-based post-processing. Speciï¬cally, we
develop a simple yet effective network structure to extract
the latent features of input low-quality underwater images.
The network cascades three residual groups for feature en-
hancement and channel attention block is incorporated into
each residual group. Afterwards, the output of the CNN
models is transformed into YUV color space for luminance
correction. Besides, we design a collaborative loss function
containing perceive loss, MSE loss, gradient loss, and SSIM
loss to preserve the texture features. Results show that our
proposed framework outperforms state-of-the-art methods both
quantitatively and qualitatively.
II. R ELATED WORK
A. Learning-based Underwater Image Enhancement
As an effective deep learning method, CNN has been widely
adopted in the ï¬eld of visual enhancement. To continuously
promote, scientists also try to apply this method to the en-
hancement of underwater images.
Due to the arduousness of obtaining high-quality underwa-
ter reference images, Cameron. [13] proposed to use Cycle
Generative Adversarial Network (CycleGAN) [14] to improve
the quality of underwater visual scenes. Similarly, introduc-
ing gradient descent Loss to enhance the predictive ability
of the generated network; Wang [12] proposed an unsu-
pervised generative opposition network (UWGAN), which
utilized aerial image, depth map pairs, and optimized imaging
models to generate realistic underwater images. Speciï¬cally,
the obtained comprehensive underwater data further used the
U-Net network to directly reconstruct the underwater clear
image, while maintaining the structural similarity of the scene
content; Li [15] constructed an Underwater Image Enhance-
ment Benchmark (UIEB) including 950 real world underwater
images, 890 of which have the corresponding reference im-
ages, and propose an underwater image enhancement network
(called Water-Net); Zhang [16] proposed an attention-based
neural network to generate high-quality enhanced low-light
images from raw sensor data; Li [17] proposed an under-
water image enhancement with image colorfulness measure.
This model encompasses a pre-processed non-parametric layer
and an adaptively reï¬ned parameter layer, optimizing the
enhancement network based on the joint of pixel level and
feature level; Li [18] proposed a lightweight convolutional
enhancement network (UWCNN) with good applicability. The
method is based on the synthetic underwater image training
set to directly reconstruct clear underwater images, which hasa good real-time characteristic and can be easily extended to
the frame-by-frame enhancement of the video.
Compared with the conventional methods, the CNN-based
methods above could achieve notable performance. Neverthe-
less, their computational complexity is greatly increased owing
to the larger cost of parameter and longer runtime than the
rule-based methods. Some networks like UWCNN has fewer
parameters than other methods like UWGAN and WaterNet,
whereas its performance is also compromised.
B. Underwater Image Dataset
Deep learning is a data-driven learning method. FUniE-
GAN opens source data set EUVP which is divided into
three subsets. These real underwater images from seven dif-
ferent camera equipment are cropped part of images from the
Youtube videos, and the ground truth is generated by a trained
CycleGAN [19]. UFO120 [20] is a new dataset divided into
training dataset and test benchmark dataset. It can be used
for model training tasks for underwater image super-division.
Li [15] provided paired dataset called UIEB where the original
image comes from the real underwater image, and the ground
truth is selected from a variety of traditional methods by
comparing. The dataset includes 890 training pictures and 90
test pictures, and small pictures are very convenient to test
and are commonly used. The author further used this dataset
to design WaterNet for underwater image enhancement. In this
paper, we perform our experiments using UIEB dataset.
III. P ROPOSED METHOD
In this section, we will describe the framework of our pro-
posed underwater image enhancement methods in details, and
introduce the loss function employed in our work. Ultimately,
we will present the post-processing employed, i.e., luminance
correction in YUV color space.
A. Network Architecture
Overall Framework. As illustrated in Fig. 1, the method
proposed consists of a single branch network and a post-
processing operation. The input image is an unprocessed RGB
underwater image of size 256 2563. Our network ï¬rst uses
3332 and 3364 convolution kernels to perform ï¬‚at
convolution operations to extract shallow features, and then
three residual groups are used to extract deeper features.
In each residual group, three channel attention blocks are
utilized to learn the distribution weights of different channels
to strengthen useful features and suppress invalid features. In
the CA block module, input data is squeezed into size of
11C data by average pooling, then this 1 1C descriptors
are put through the convolutional layers and activated by
sigmoid function. The output of the third residual group is
25625616. After processing by the convolution kernel with
a size of 33, an enhanced image is generated with a size of
2562563.
Channel attention module With notable breakthroughs in
both image and natural language processing, the AttentionFig. 1: Structure of our proposed network which concatenates three residual groups. Each residual group has three channel
attention blocks for latent feature extraction and texture preservation.
mechanism has been substantiated in improving network per-
formance. Drawing on the human brain and the human eye
perception mechanism, the attention mechanism intelligently
focuses attention on the local information of interest and
neglects extraneous information as much as CNN could. The
mechanism was proposed in 2018 by Jie Hu [21] and
extensively utilised in deep learning scenarios.
Fig. 2: Channel attention module used in our residual group.
As demonstrated in Fig. 2, H,W, and Care height, width,
and the number of channels, respectively. ris the reduction
factor. At ï¬rst, the input of size H WC features is processed
to generate channel weight of 1 1 for each channel through
pooling operation. Next, through using the convolutional and
sigmoid layers, the network learns the degree of dependence
across channels to obtain the weight of each channel. Finally,
the feature map of each channel with the corresponding
weighted value will be multiplied together to generate the
adjusted feature map of the same size as the input.
B. Loss Function
To ensure the generation of high-quality visual images, we
adopt a collaborative loss function in our work: content percep-
tion loss, mean square error(MSE) loss, structural similarity
(SSIM) loss, and gradient descent loss. The ï¬nal loss function
is presented as:
L=wvggLvgg+wmseLmse
+wssimLssim+wgdlLgdl;(1)wherewindicates the weighting factor.
Lvggis a content loss item which drives the network to
get training results with similar content. The ReLU activation
layer based on the pre-trained 19-layer VGG network deï¬nes
Lvggas:
Lvgg(E;G) =1
CjHjWjNX
i=1jj'j(Ei) 'j(Gi)jj;(2)
whereEis the enhanced image and Gis the reference image.
Ndenotes batch size, and Cj,Hj,Wjindicate the number,
height, and width of feature maps, respectively.
Lmse is the MSE loss between the enhanced image and
the reference image. Let nandmrepresent the width and
height of the image, respectively. We use Lmse to calculate
the average pixel-wise distance on the enhanced image and
the ground truth. It is written as:
Lmse=1
mnm 1X
i=0n 1X
j=0jjE(i;j) G(i;j)jj2: (3)
Lssim denotes the SSIM loss between enhanced image and
reference image:
SSIM (E;G) =(2EG+c1)(EG+c2)
(2
E+2
G+c1)(2
E+2
G+c2)(4)
Lssim(E;G) = 1 SSIM (E;G) (5)
whereE,Grepresents the average of image EandG,
respectively, E,Gmeans standard deviation of E and G,
EGmeans the covariance of E and G, c1andc2are constant
to avoid situations where the denominator is 0.
Lgdlis the gradient descent loss which introduces enhanced
image, reference image, and correlation between adjacent
pixels.means an integer greater than or equal to 1, in ourexperiment, we set to 1. The relevant formula is expressed
as follows:
Lgdl=X
i;jjjGi;j Gi 1;jj jEi;j Ei 1;jjj
+X
i;jjjGi;j Gi;j 1j jEi;j Ei;j 1jj:(6)
C. Post-processing in YUV Color Space
Our proposed method adds a post-processing module to
improve the overall brightness of the image generated by
the network. To elaborate further, we convert the original
image from RGB space to YUV space. YUV includes three
components Y , U, and V . Y stands for luma, U and V stand
for chroma respectively. Then we perform maximum and
minimum normalization operation on the Y component and
convert the original data linearization method to the range of
[0, 1]. The corresponding formula is as follows, MAX and
MIN in the formula respectively represent the maximum and
minimum of the Y channel component:
Y0= 255:0Y MIN
MAX MIN: (7)
IV. E XPERIMENTAL RESULTS
A. Implementation Details
The dataset of our project is mainly based on UIEBâ€™s
890 high-deï¬nition underwater images with corresponding
reference images. Speciï¬cally, 800 are randomly selected as
the training set, while the remaining 90 images along with
10 images randomly selected from the UFO120 dataset [20]
constitute the test set. We resized the original picture and
cropped them to size 256 256 for better training and test. We
train the model use Adam, set the segmented learning rate from
0.001, and set the batch size to 16 and epoch to 500. We use
Tensorï¬‚ow as the deep learning framework on an Inter(R)i7-
9700k CPU@3.60GHz, 32GB RAM, and a NVIDIA GeForce
RTX 2080Ti GPU. The total training time is around 10 hours.
B. Evaluation Metrics
We employ Peak Signal-to-Noise Ratio (PSNR) and Struc-
tural Similarity (SSIM) to measure the image quality, and use
Underwater Image Quality Measure (UIQM) [22] to evaluate
non-reference underwater quality assessments. The deï¬nition
of PSNR can be expressed as:
PSNR = 10log10(Max2
i
MSE); (8)
whereMax2
imeans the max pixel value of image provided
ranging from 0 to 255 for uint8 data type. For any picture,
PSNR reï¬‚ects the degree of distortion of the image, and SSIM
objectively reï¬‚ects how much the image is structural similarity
on scale. For the particularity of underwater image quality
assessment, the predecessors proposed UIQM to evaluate the
overall underwater image quality, which can intuitively reï¬‚ect
image color, sharpness and contrast. UIQM is a reference-freeTABLE I: Performance comparison with state-of-the-art meth-
ods
Method SSIM PSNR (dB) UIQM
IBLA [8] 0.6672 15.5341 1.7402
DCP [6] 0.6657 13.5033 1.9462
UDCP [23] 0.5349 12.0763 1.7544
HE [24] 0.7453 15.6528 2.6197
CLAHE [25] 0.8218 18.2440 2.8704
ULAP [9] 0.6870 15.1370 1.7686
UWGAN [12] 0.7937 17.0210 3.1170
UWCNN [26] 0.7179 15.5202 2.8419
WaterNet [15] 0.8000 18.7366 2.7165
Ours 0.8720 21.9273 3.0522
index based on human visual system excitation. Aimed at the
degradation of underwater images, the UIQM is expressed as
a linear combination of three parts: Color Measurement index
(UICM), Degree Measurement Index (UISM), and Contrast
Measurement Index (UIConM). The calculation of UIQM is
as follows:
UIQM =c1UICM +c2UISM +c3UIconM; (9)
wherec1,c2, andc3indicate the weighting factors.
UICM = 0:0268q
2
;RG +2
;YB
+ 0:1586q
2
;RG+2
;YB(10)
UISM = 3
c=1cEME (grayscaleedge c) (11)
EME =2
k1k2k1
l=1k2
k=1log(Imax;k;l
Imin;k;l) (12)
UIConM = logAMEE (Intensity )
=1
k1k2O
k1
l=1k2
k=1(Imax;k;l Imin;k;l
Imax;k;lLImin;k;l)(13)
In UICM calculation, ;RG , and;YB mean asymmetri-
cal trimmed chroma intensity average in RG and YB. 2
;RG
and2
;RG mean the pixel variance in RG and YB.
In UISM calculation, grayscale edge representative gray
edge map of each R,G,B,EME is a method of measuring
edge sharpness and the values of the linear combination
areR=0.299,G=0.587,B=0.114. In EME, k1,k2mean
number of image divisions andImax;k;l
Imin;j;lmeans the relative
contrast.
In UIConM, k1,k2mean number of image divisions andN,Landmean the special operations on images.
C. quantitative comparison
We compare our work with ten state-of-the-art methods, as
shown in Table. I. Findings from this project indicate that our
proposed method achieves the highest SSIM and PSNR of all,
signiï¬cantly improves the SSIM, PSNR, and UIQM metrics,
e.g. For instance, the average SSIM, PSNR, and UIQM of
recent WaterNet is 0.8000, 18.7366dB, 2.7165, whereas ours
is as high as 0.8720, 21.9273, and 3.0522.(a) Underwater scenes with creatures
(b) Underwater scenes with yellowish tone
(c) Underwater scenes with bluenish tone
(d) Underwater scenes with greenish tone
(e) Low-light illumination underwater scenes
Fig. 3: Visualization results of different methods in low-light illumination underwater scenes.(a) Input Raw underwater images
(b) Underwater images enhanced without post-processing
(c) Underwater images enhanced with post-processing
(d) Ground truth underwater images
Fig. 4: Visual quality comparison of using and without using
post-processing in our approach.
D. Qualitative Comparison
The enhanced images of different methods are visualized in
Fig. 3 for comparison. In the ï¬gure, the last column provides
the ground truth images for reference. There are several diverse
underwater scenes such as (a) Scenes with creatures, (b)
Yellowish scenes. (c) Blueish scenes. (d) Greenish scenes.
(e) low-light illumination scenes. UW-GAN, UWCNN, and
WaterNet are three state-of-the-art CNN-based methods. It is
shown in the ï¬gure that UW-GAN fails in blurring removal,
color cast correcting, and chroma and brightness restoring,
e.g., in Fig. 3d, the visualization of UW-GAN is unclear
and unnatural. UWCNN is a simple network with a small
number of parameters. The images enhanced by UWCNN have
distinct color cast (Fig. 4c and Fig. 4d) and blurry details
(Fig. 3d). Among all the other methods, WaterNet and ours
conduct more efï¬cient in visual quality, yet WaterNet performs
worse than ours, especially in scenes like Yellowish and low-
light scenes. By contrast, the underwater images enhanced by
our method are much closer to the ground truth and visually
pleasing. Ultimately, the qualitative comparison proves again
the superiority of our proposed method.
E. Ablation Study
In order to verify the contribution of each stage in our
proposed framework, we conduct ablative experiments on theloss function and post-processing respectively.
1) Loss function: To verify the effectiveness of the percep-
tual loss, gradient loss, structural similarity loss, and mean
square error loss used in our method, we conduct a series of
ablation experiments on the four loss functions respectively.
Each time a certain loss function item is disabled. The exper-
imental results are shown in Table II.
TABLE II: Loss function ablation experiment
Method SSIM PSNR (dB) UIQM
loss in ours 0.8720 21.9273 3.0522
w/oLvggloss 0.8544 20.2642 3.0068
w/oLmse loss 0.8242 19.8349 3.1453
w/oLssim loss 0.8151 19.8073 3.0888
w/oLgdlloss 0.8651 21.3996 3.0661
Comparing the data Table II, we ï¬nd that the loss proposed
can obtain the highest SSIM and PSNR. Without using Lgdl,
the UIQM value is slightly improved at the expense of
decreasing UIQM and SSIM. Without using Lmse, nothing
could be higher with a dramatic decline except UIQM. In
general, our proposed collaborative loss successfully reach a
balance across SSIM, PSNR, and UIQM, leading to high-
quality underwater images.
2) Post-processing: In order to verify the contribution of
the post-processing operation, we conduct a post-processing
ablation experiment in which the post-processing is removed.
We can see from Table. III that without the post-processing
operation, both SSIM and PSNR are greatly decreased al-
though there is a slight decline in UIQM. We also visualize the
results in Fig. 4. It is obvious that using the post-processing
signiï¬cantly improves the the sharpness and brightness of the
images.
TABLE III: Efï¬ciency comparison experiment results
Method SSIM PSNR (dB) UIQM
w/o post-processing 0.8606 21.3234 3.0963
Ours 0.8720 21.9273 3.0522
F . Computational Complexity
In addition to the quantitative and qualitative comparisons
above, we analyze the number of parameters and the runtime
of these CNN-based methods for comparison, as shown in Ta-
ble IV. Although UWCNN has fewer parameters and UWGAN
has less runtime than ours, their performance is much lower
than ours. The number of parameters in WaterNet is 1106k,
which is 2.5 times our number. The runtime of WaterNet is
also 3 times that of ours. Instead, our method reaches the
highest performance of all with low complexity cost.
V. C ONCLUSION
This paper proposes an efï¬cient, low-complexity framework
for underwater image quality enhancement. Our frameworkTABLE IV: computational complexity comparison with exist-
ing CNN-based methods
Method Parameter Runtime
UWCNN [18] 97901 0.2894
UWGAN [12] 1939194 0.0170
WaterNet [15] 1106274 0.6096
Ours 436682 0.2025
consists of two stages: CNN-based enhancement and post-
processing operation. In terms of the CNN-based enhance-
ment, we design a CNN structure using cascading residual
groups for feature extraction and enhancement. In each resid-
ual group, the channel attention mechanism is incorporated
to self-adjust the channel weight coefï¬cients of each channel.
We observe that the images generated from CNN are of low
contrast. To this end, we feed the images to the post-processing
stage for further brightness improvement. Speciï¬cally, the
image is transformed into YUV space and the Y component is
normalized and corrected. We conduct our experiments on the
conventional UIEB dataset. Results demonstrate that compared
with state-of-the-art methods, our proposed approach achieves
the highest SSIM and PSNR values of all. Besides, we also
compare the visual quality of all methods. Results conï¬rm that
the underwater images enhanced by our approach look much
better than those of other methods and much closer to the
ground truth. In terms of the computational complexity, our
method costs only 436k parameters and our runtime is only
1=3of existing method WaterNet.
REFERENCES
[1] C.-Y . Li, J.-C. Guo, R.-M. Cong, Y .-W. Pang, and B. Wang, â€œUnderwater
image enhancement by dehazing with minimum information loss and
histogram distribution prior,â€ IEEE Transactions on Image Processing ,
vol. 25, no. 12, pp. 5664â€“5677, 2016.
[2] C. Ancuti, C. O. Ancuti, T. Haber, and P. Bekaert, â€œEnhancing underwa-
ter images and videos by fusion,â€ in 2012 IEEE conference on computer
vision and pattern recognition . IEEE, 2012, pp. 81â€“88.
[3] B. McGlamery, â€œA computer model for underwater camera systems,â€
inOcean Optics VI , vol. 208. International Society for Optics and
Photonics, 1980, pp. 221â€“231.
[4] J. S. Jaffe, â€œComputer modeling and the design of optimal underwater
imaging systems,â€ IEEE Journal of Oceanic Engineering , vol. 15, no. 2,
pp. 101â€“111, 1990.
[5] N. Carlevaris-Bianco, A. Mohan, and R. M. Eustice, â€œInitial results in
underwater single image dehazing,â€ in Oceans 2010 Mts/IEEE Seattle .
IEEE, 2010, pp. 1â€“8.
[6] K. He, J. Sun, and X. Tang, â€œSingle image haze removal using dark
channel prior,â€ IEEE transactions on pattern analysis and machine
intelligence , vol. 33, no. 12, pp. 2341â€“2353, 2010.
[7] H. Lu, Y . Li, L. Zhang, and S. Serikawa, â€œContrast enhancement for
images in turbid water,â€ JOSA A , vol. 32, no. 5, pp. 886â€“893, 2015.[8] Y .-T. Peng and P. C. Cosman, â€œUnderwater image restoration based
on image blurriness and light absorption,â€ IEEE transactions on image
processing , vol. 26, no. 4, pp. 1579â€“1594, 2017.
[9] W. Song, Y . Wang, D. Huang, and D. Tjondronegoro, â€œA rapid scene
depth estimation model based on underwater light attenuation prior for
underwater image restoration,â€ in Paciï¬c Rim Conference on Multimedia .
Springer, 2018, pp. 678â€“688.
[10] X. Chen, J. Yu, S. Kong, Z. Wu, X. Fang, and L. Wen, â€œTowards quality
advancement of underwater machine vision with generative adversarial
networks,â€ 2018.
[11] J. Li, K. A. Skinner, R. M. Eustice, and M. Johnson-Roberson, â€œWater-
gan: Unsupervised generative network to enable real-time color correc-
tion of monocular underwater images,â€ IEEE Robotics and Automation
letters , vol. 3, no. 1, pp. 387â€“394, 2017.
[12] N. Wang, Y . Zhou, F. Han, H. Zhu, and J. Yao, â€œUwgan: underwater
gan for real-world underwater color restoration and dehazing,â€ arXiv
preprint arXiv:1912.10269 , 2019.
[13] C. Fabbri, M. J. Islam, and J. Sattar, â€œEnhancing underwater imagery
using generative adversarial networks,â€ in 2018 IEEE International
Conference on Robotics and Automation (ICRA) . IEEE, 2018, pp.
7159â€“7165.
[14] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, â€œUnpaired image-to-image
translation using cycle-consistent adversarial networks,â€ in Proceedings
of the IEEE international conference on computer vision , 2017, pp.
2223â€“2232.
[15] C. Li, C. Guo, W. Ren, R. Cong, J. Hou, S. Kwong, and D. Tao, â€œAn
underwater image enhancement benchmark dataset and beyond,â€ IEEE
Transactions on Image Processing , vol. 29, pp. 4376â€“4389, 2019.
[16] C. Zhang, Q. Yan, Y . Zhu, X. Li, J. Sun, and Y . Zhang, â€œAttention-based
network for low-light image enhancement,â€ in 2020 IEEE International
Conference on Multimedia and Expo (ICME) . IEEE, 2020, pp. 1â€“6.
[17] H. Li, X. Yang, Z. Li, and T. Zhang, â€œUnderwater image enhancement
with image colorfulness measure,â€ arXiv preprint arXiv:2004.08609 ,
2020.
[18] C. Li, S. Anwar, and F. Porikli, â€œUnderwater scene prior inspired deep
underwater image and video enhancement,â€ Pattern Recognition , vol. 98,
p. 107038, 2020.
[19] Q. You, C. Wan, J. Sun, J. Shen, H. Ye, and Q. Yu, â€œFundus image
enhancement method based on cyclegan,â€ in 2019 41st annual inter-
national conference of the IEEE engineering in medicine and biology
society (EMBC) . IEEE, 2019, pp. 4500â€“4503.
[20] M. J. Islam, P. Luo, and J. Sattar, â€œSimultaneous enhancement and super-
resolution of underwater imagery for improved visual perception,â€ arXiv
preprint arXiv:2002.01155 , 2020.
[21] J. Hu, L. Shen, and G. Sun, â€œSqueeze-and-excitation networks,â€ in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2018, pp. 7132â€“7141.
[22] K. Panetta, C. Gao, and S. Agaian, â€œHuman-visual-system-inspired
underwater image quality measures,â€ IEEE Journal of Oceanic Engi-
neering , vol. 41, no. 3, pp. 541â€“551, 2015.
[23] P. L. Drews, E. R. Nascimento, S. S. Botelho, and M. F. M. Campos,
â€œUnderwater depth estimation and image restoration based on single
images,â€ IEEE computer graphics and applications , vol. 36, no. 2, pp.
24â€“35, 2016.
[24] R. Hummel, â€œImage enhancement by histogram transformation,â€ Un-
known , 1975.
[25] S. M. Pizer, â€œContrast-limited adaptive histogram equalization: Speed
and effectiveness stephen m. pizer, r. eugene johnston, james p. ericksen,
bonnie c. yankaskas, keith e. muller medical image display research
group,â€ in Proceedings of the First Conference on Visualization in
Biomedical Computing, Atlanta, Georgia , vol. 337, 1990.
[26] Y . Guo, H. Li, and P. Zhuang, â€œUnderwater image enhancement using
a multiscale dense generative adversarial network,â€ IEEE Journal of
Oceanic Engineering , vol. 45, no. 3, pp. 862â€“870, 2019."
ç•™å®¿ä½è¯ææ–™.docx,D:/tool/Pycharm/summerProject/examples\ç•™å®¿ä½è¯ææ–™.docx,"ç•™å®¿ä½è¯ææ–™
ï¼ˆå‚åŠ å­¦ç§‘ç«èµ›ã€ç§‘ç ”é¡¹ç›®ã€å­¦æ ¡ç»„ç»‡çš„åœ¨æ­ç¤¾ä¼šå®è·µå­¦ç”Ÿç”¨ï¼‰

å…¹æœ‰æ­å·å¸ˆèŒƒå¤§å­¦ä¿¡æ¯ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢           ç­‰åŒå­¦ï¼ˆå®Œæ•´åå•è§é™„ä»¶ï¼‰ï¼Œæš‘å‡å›                     ï¼Œç¡®éœ€ç•™å®¿åœ¨æ ¡ï¼Œç•™å®¿æ—¶é—´ä¸º     æœˆ    æ—¥è‡³    æœˆ    æ—¥ï¼Œæƒ…å†µå±å®ã€‚
    ç‰¹æ­¤è¯æ˜ã€‚


æŒ‡å¯¼æ•™å¸ˆï¼ˆç­¾åï¼‰ï¼š                 
    è”ç³»æ–¹å¼ï¼š                 



"
